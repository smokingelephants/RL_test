{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "RqxatIwPOXe_"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMR6r6UlvgzGPROD7Ovt2Fp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smokingelephants/RL_test/blob/master/Custom_Maze_env_DQN_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQMN8S9N96lX",
        "outputId": "2b60d78a-6794-4eed-81d5-d6b437174530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==0.26.3 in /usr/local/lib/python3.10/dist-packages (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.26.3) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.26.3) (2.2.1)\n",
            "Requirement already satisfied: gymnasium-notices>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.26.3) (0.0.1)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Using cached gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Requirement already satisfied: shimmy[atari]~=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.10/dist-packages (from shimmy[atari]~=1.3.0->stable-baselines3[extra]) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.62.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra]) (12.4.99)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Installing collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: Gymnasium 0.26.3\n",
            "    Uninstalling Gymnasium-0.26.3:\n",
            "      Successfully uninstalled Gymnasium-0.26.3\n",
            "Successfully installed gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install gymnasium==0.26.3\n",
        "# !pip install stable-baselines3[extra]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqxatIwPOXe_"
      },
      "source": [
        "##  Gym env skeleton\n",
        "\n",
        "In practice this is how a gym environment looks like.\n",
        "Here, we have implemented a simple grid world were the agent must learn to go always left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "rYzDXA9vJfz1"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import gymnasium as gym\n",
        "# from gymnasium import spaces\n",
        "\n",
        "\n",
        "# class GoLeftEnv(gym.Env):\n",
        "#     \"\"\"\n",
        "#     Custom Environment that follows gym interface.\n",
        "#     This is a simple env where the agent must learn to go always left.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "#     metadata = {\"render_modes\": [\"console\"]}\n",
        "\n",
        "#     # Define constants for clearer code\n",
        "#     LEFT = 0\n",
        "#     RIGHT = 1\n",
        "\n",
        "#     def __init__(self, grid_size=10, render_mode=\"console\"):\n",
        "#         super(GoLeftEnv, self).__init__()\n",
        "#         self.render_mode = render_mode\n",
        "\n",
        "#         # Size of the 1D-grid\n",
        "#         self.grid_size = grid_size\n",
        "#         # Initialize the agent at the right of the grid\n",
        "#         self.agent_pos = grid_size - 1\n",
        "\n",
        "#         # Define action and observation space\n",
        "#         # They must be gym.spaces objects\n",
        "#         # Example when using discrete actions, we have two: left and right\n",
        "#         n_actions = 2\n",
        "#         self.action_space = spaces.Discrete(n_actions)\n",
        "#         # The observation will be the coordinate of the agent\n",
        "#         # this can be described both by Discrete and Box space\n",
        "#         self.observation_space = spaces.Box(\n",
        "#             low=0, high=self.grid_size, shape=(1,), dtype=np.float32\n",
        "#         )\n",
        "\n",
        "#     def reset(self, seed=None, options=None):\n",
        "#         \"\"\"\n",
        "#         Important: the observation must be a numpy array\n",
        "#         :return: (np.array)\n",
        "#         \"\"\"\n",
        "#         super().reset(seed=seed, options=options)\n",
        "#         # Initialize the agent at the right of the grid\n",
        "#         self.agent_pos = self.grid_size - 1\n",
        "#         # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "#         return np.array([self.agent_pos]).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "#     def step(self, action):\n",
        "#         if action == self.LEFT:\n",
        "#             self.agent_pos -= 1\n",
        "#         elif action == self.RIGHT:\n",
        "#             self.agent_pos += 1\n",
        "#         else:\n",
        "#             raise ValueError(\n",
        "#                 f\"Received invalid action={action} which is not part of the action space\"\n",
        "#             )\n",
        "\n",
        "#         # Account for the boundaries of the grid\n",
        "#         self.agent_pos = np.clip(self.agent_pos, 0, self.grid_size)\n",
        "\n",
        "#         # Are we at the left of the grid?\n",
        "#         terminated = bool(self.agent_pos == 0)\n",
        "#         truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "#         # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "#         reward = 1 if self.agent_pos == 0 else 0\n",
        "\n",
        "#         # Optionally we can pass additional info, we are not using that for now\n",
        "#         info = {}\n",
        "\n",
        "#         return (\n",
        "#             np.array([self.agent_pos]).astype(np.float32),\n",
        "#             reward,\n",
        "#             terminated,\n",
        "#             truncated,\n",
        "#             info,\n",
        "#         )\n",
        "\n",
        "#     def render(self):\n",
        "#         # agent is represented as a cross, rest as a dot\n",
        "#         if self.render_mode == \"console\":\n",
        "#             print(\".\" * self.agent_pos, end=\"\")\n",
        "#             print(\"x\", end=\"\")\n",
        "#             print(\".\" * (self.grid_size - self.agent_pos))\n",
        "\n",
        "#     def close(self):\n",
        "#         pass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Maze environment"
      ],
      "metadata": {
        "id": "2LvWAan9Pxq-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "m1phUf9IP1Wm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import math\n",
        "from numpy.random import random_integers as rnd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class MazeEnv_new(gym.Env):\n",
        "    \"\"\"\n",
        "    Custom Environment that follows gym interface.\n",
        "    This is a simple env where the agent must learn to go always left.\n",
        "    \"\"\"\n",
        "\n",
        "    # Because of google colab, we cannot implement the GUI ('human' render mode)\n",
        "    metadata = {\"render_modes\": [\"None\"]}\n",
        "\n",
        "    # Define constants for clearer code\n",
        "    UP = 0\n",
        "    DOWN = 1\n",
        "    LEFT = 2\n",
        "    RIGHT = 3\n",
        "\n",
        "    #True are obstacles\n",
        "    def make_maze(self, width=25, height=25, complexity=.05, density =.05): #.75 .75 #.75 .05\n",
        "        # Only odd shapes\n",
        "        shape = ((height//2)*2+1, (width//2)*2+1)\n",
        "        # Adjust complexity and density relative to maze size\n",
        "        complexity = int(complexity*(5*(shape[0]+shape[1])))\n",
        "        density    = int(density*(shape[0]//2*shape[1]//2))\n",
        "        # Build actual maze\n",
        "        Z = np.zeros(shape, dtype=bool)\n",
        "        # Fill borders\n",
        "        Z[0,:] = Z[-1,:] = 1\n",
        "        Z[:,0] = Z[:,-1] = 1\n",
        "        # Make isles\n",
        "        for i in range(density):\n",
        "            x, y = rnd(0,shape[1]//2)*2, rnd(0,shape[0]//2)*2\n",
        "            Z[y,x] = 1\n",
        "            for j in range(complexity):\n",
        "                neighbours = []\n",
        "                if x > 1:           neighbours.append( (y,x-2) )\n",
        "                if x < shape[1]-2:  neighbours.append( (y,x+2) )\n",
        "                if y > 1:           neighbours.append( (y-2,x) )\n",
        "                if y < shape[0]-2:  neighbours.append( (y+2,x) )\n",
        "                if len(neighbours):\n",
        "                    y_,x_ = neighbours[rnd(0,len(neighbours)-1)]\n",
        "                    if Z[y_,x_] == 0:\n",
        "                        Z[y_,x_] = 1\n",
        "                        Z[y_+(y-y_)//2, x_+(x-x_)//2] = 1\n",
        "                        x, y = x_, y_\n",
        "        return Z, width, height\n",
        "\n",
        "    def setup_maze(self, width=25, height=25):\n",
        "        # get maze and the Size of the adjusted 2D-grid\n",
        "        self.maze, self.width, self.height = self.make_maze(width=width, height=height)\n",
        "        #self.start_loc = np.array([0,0])\n",
        "        x,y = np.where(self.maze == True)\n",
        "        non_term_vars = np.zeros_like(x)\n",
        "        self.obstacles = np.squeeze(np.array([[x],[y],[non_term_vars]])).T\n",
        "        x,y = np.where(self.maze == False)\n",
        "        non_term_vars = np.zeros_like(x)\n",
        "        self.valid_points = np.squeeze(np.array([[x],[y],[non_term_vars]])).T\n",
        "        self.terminal_points = np.squeeze(np.array([[x],[y],[1-non_term_vars]])).T\n",
        "        self.start_loc = self.valid_points[0]\n",
        "        self.goal_loc = self.valid_points[-1]\n",
        "\n",
        "    def __init__(self, width=25, height=25, render_mode=\"console\"):\n",
        "        super(MazeEnv_new, self).__init__()\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "        self.setup_maze(width=width, height=height)\n",
        "\n",
        "        # Initialize the agent at the right of the grid\n",
        "        self.current_state = self.start_loc #for now\n",
        "\n",
        "        # Define action and observation space\n",
        "        # They must be gym.spaces objects\n",
        "        # Example when using discrete actions, we have two: left and right\n",
        "        self.n_actions = 4\n",
        "        self.action_space = spaces.Discrete(self.n_actions)\n",
        "        # The observation will be the coordinate of the agent\n",
        "        # this can be described both by Discrete and Box space\n",
        "        max_grid_size = np.max([self.width,self.height])\n",
        "        #print(max_grid_size)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=max_grid_size, shape=(3,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def valid(self, state):\n",
        "        nx,ny = state[0], state[1]\n",
        "        return( 0 <= nx < self.height and 0 <= ny < self.width and self.maze[nx][ny] == False )\n",
        "\n",
        "    def is_goal(self):\n",
        "        # game 1\n",
        "        # reach a particular goal, reach last state in valid loc array, for now\n",
        "        result = bool((self.current_state==self.goal_loc).all())\n",
        "        r = np.random.rand()\n",
        "        if r > 0.999:\n",
        "          result = True\n",
        "        return result\n",
        "\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        \"\"\"\n",
        "        Important: the observation must be a numpy array\n",
        "        :return: (np.array)\n",
        "        \"\"\"\n",
        "        super().reset(seed=seed, options=options)\n",
        "        # Initialize the agent at the right of the grid\n",
        "        #self.current_state = self.start_loc\n",
        "        self.current_state = self.valid_points[np.random.randint(len(self.valid_points))]\n",
        "        # here we convert to float32 to make it more general (in case we want to use continuous actions)\n",
        "        return np.array(self.current_state).astype(np.float32), {}  # empty info dict\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        self.next_state = np.array(self.current_state)\n",
        "        self.slip = 0.1\n",
        "        r = np.random.rand()\n",
        "        if r < self.slip:\n",
        "            action = np.random.randint(self.n_actions)\n",
        "        if action == self.UP:\n",
        "            self.next_state[0] = self.next_state[0] - 1\n",
        "        elif action == self.DOWN:\n",
        "            self.next_state[0] = self.next_state[0] + 1\n",
        "        elif action == self.LEFT:\n",
        "            self.next_state[1] = self.next_state[1] - 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.next_state[1] = self.next_state[1] + 1\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Received invalid action={action} which is not part of the action space\"\n",
        "            )\n",
        "\n",
        "        # Account for the boundaries of the grid\n",
        "        if not self.valid(self.next_state):\n",
        "            self.next_state = np.array(self.current_state)\n",
        "            reward = -5.0\n",
        "        else:\n",
        "            reward = -np.random.rand()\n",
        "\n",
        "        # Are we at the left of the grid?\n",
        "        terminated = self.is_goal()\n",
        "        truncated = False  # we do not limit the number of steps here\n",
        "\n",
        "        # Null reward everywhere except when reaching the goal (left of the grid)\n",
        "        if self.is_goal():\n",
        "            reward = 1000.0\n",
        "\n",
        "        # Optionally we can pass additional info, we are not using that for now\n",
        "        info = {}\n",
        "\n",
        "        self.current_state = np.array(self.next_state)\n",
        "        return (\n",
        "            np.array(self.next_state).astype(np.float32),\n",
        "            reward,\n",
        "            terminated,\n",
        "            truncated,\n",
        "            info,\n",
        "        )\n",
        "\n",
        "    def render(self):\n",
        "      # agent is represented as a cross, rest as a dot\n",
        "        gym.logger.warn(\n",
        "            \"Render mode not implemented\"\n",
        "        )\n",
        "        return\n",
        "\n",
        "    def show_path(self, points, color = 'r', name = 'curr', stayAwake = False):\n",
        "        plt.imshow(self.maze, cmap=plt.cm.binary,interpolation='nearest')\n",
        "        plt.scatter(points[:,1], points[:,0], s=5, c=color)  #s=7\n",
        "        #plt.savefig(name + '_maze_path.png')\n",
        "\n",
        "        if not stayAwake:\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([[]])\n",
        "b = np.array(a)\n",
        "a[2] = 10\n",
        "print(a,b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PmWuqClPxMN",
        "outputId": "60d8a6ef-47c3-4baf-b94c-3bea67e7c35d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 1  2 10  4] [1 2 3 4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy5mlho1-Ine"
      },
      "source": [
        "### Validate the environment\n",
        "\n",
        "Stable Baselines3 provides a [helper](https://stable-baselines3.readthedocs.io/en/master/common/env_checker.html) to check that your environment follows the Gym interface. It also optionally checks that the environment is compatible with Stable-Baselines (and emits warning if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9DOpP_B0-LXm"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_checker import check_env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "1CcUVatq-P0l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "9be2acc7-6c99-46a1-a643-ad9d34170176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-0d4da59cd8ed>:37: DeprecationWarning: This function is deprecated. Please call randint(0, 50 + 1) instead\n",
            "  x, y = rnd(0,shape[1]//2)*2, rnd(0,shape[0]//2)*2\n",
            "<ipython-input-108-0d4da59cd8ed>:46: DeprecationWarning: This function is deprecated. Please call randint(0, 3 + 1) instead\n",
            "  y_,x_ = neighbours[rnd(0,len(neighbours)-1)]\n",
            "<ipython-input-108-0d4da59cd8ed>:46: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
            "  y_,x_ = neighbours[rnd(0,len(neighbours)-1)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# env = GoLeftEnv()\n",
        "env = MazeEnv_new(width=100, height=100)\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)\n",
        "env.show_path(env.valid_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ3khFtkSE0g"
      },
      "source": [
        "### Testing the environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "i62yf2LvSAYY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c5ebb6bd-d824-4847-dfe8-586142500157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-0d4da59cd8ed>:166: UserWarning: \u001b[33mWARN: Render mode not implemented\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box(0.0, 100.0, (3,), float32)\n",
            "Discrete(4)\n",
            "2\n",
            "Step 1\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.7458456672154163 done= False\n",
            "Step 2\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.9086645482479843 done= False\n",
            "Step 3\n",
            "Action up\n",
            "obs= [ 6. 21.  0.] reward= -0.9639887322996373 done= False\n",
            "Step 4\n",
            "Action left\n",
            "obs= [ 6. 20.  0.] reward= -0.1371386881162301 done= False\n",
            "Step 5\n",
            "Action down\n",
            "obs= [ 7. 20.  0.] reward= -0.7321689570027544 done= False\n",
            "Step 6\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.3150488929693337 done= False\n",
            "Step 7\n",
            "Action up\n",
            "obs= [ 6. 19.  0.] reward= -0.16096202334856635 done= False\n",
            "Step 8\n",
            "Action up\n",
            "obs= [ 5. 19.  0.] reward= -0.8709625845172845 done= False\n",
            "Step 9\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.9208353612984147 done= False\n",
            "Step 10\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.6356013304911833 done= False\n",
            "Step 11\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -5.0 done= False\n",
            "Step 12\n",
            "Action left\n",
            "obs= [ 5. 16.  0.] reward= -0.5032331550724596 done= False\n",
            "Step 13\n",
            "Action right\n",
            "obs= [ 5. 17.  0.] reward= -0.23706571686340605 done= False\n",
            "Step 14\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -5.0 done= False\n",
            "Step 15\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.6216912310480149 done= False\n",
            "Step 16\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -0.599102603694046 done= False\n",
            "Step 17\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.12914120640125493 done= False\n",
            "Step 18\n",
            "Action right\n",
            "obs= [ 6. 18.  0.] reward= -0.996570901309372 done= False\n",
            "Step 19\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -0.5018325744722953 done= False\n",
            "Step 20\n",
            "Action right\n",
            "obs= [ 6. 18.  0.] reward= -0.7714009834195201 done= False\n",
            "Step 21\n",
            "Action right\n",
            "obs= [ 6. 19.  0.] reward= -0.5629818032547207 done= False\n",
            "Step 22\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.23012129293267292 done= False\n",
            "Step 23\n",
            "Action right\n",
            "obs= [ 6. 21.  0.] reward= -0.5837359709902806 done= False\n",
            "Step 24\n",
            "Action down\n",
            "obs= [ 7. 21.  0.] reward= -0.2968386601344962 done= False\n",
            "Step 25\n",
            "Action right\n",
            "obs= [ 7. 22.  0.] reward= -0.397267277793522 done= False\n",
            "Step 26\n",
            "Action down\n",
            "obs= [ 8. 22.  0.] reward= -0.4162519695116642 done= False\n",
            "Step 27\n",
            "Action left\n",
            "obs= [ 8. 21.  0.] reward= -0.8940277986568897 done= False\n",
            "Step 28\n",
            "Action left\n",
            "obs= [ 8. 20.  0.] reward= -0.6379685348423325 done= False\n",
            "Step 29\n",
            "Action left\n",
            "obs= [ 8. 19.  0.] reward= -0.27776900214665246 done= False\n",
            "Step 30\n",
            "Action up\n",
            "obs= [ 7. 19.  0.] reward= -0.46040610201928267 done= False\n",
            "Step 31\n",
            "Action left\n",
            "obs= [ 7. 18.  0.] reward= -0.7094138531794018 done= False\n",
            "Step 32\n",
            "Action down\n",
            "obs= [ 8. 18.  0.] reward= -0.5629608269287693 done= False\n",
            "Step 33\n",
            "Action down\n",
            "obs= [ 9. 18.  0.] reward= -0.08546912817874097 done= False\n",
            "Step 34\n",
            "Action left\n",
            "obs= [ 9. 17.  0.] reward= -0.08260647658666465 done= False\n",
            "Step 35\n",
            "Action left\n",
            "obs= [ 9. 16.  0.] reward= -0.13771575436830819 done= False\n",
            "Step 36\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.4121804559360599 done= False\n",
            "Step 37\n",
            "Action down\n",
            "obs= [10. 17.  0.] reward= -0.39270517467192434 done= False\n",
            "Step 38\n",
            "Action down\n",
            "obs= [10. 18.  0.] reward= -0.6049064751292352 done= False\n",
            "Step 39\n",
            "Action right\n",
            "obs= [10. 19.  0.] reward= -0.9573391273374645 done= False\n",
            "Step 40\n",
            "Action up\n",
            "obs= [ 9. 19.  0.] reward= -0.3210027142245441 done= False\n",
            "Step 41\n",
            "Action right\n",
            "obs= [ 9. 20.  0.] reward= -0.012787851450110077 done= False\n",
            "Step 42\n",
            "Action left\n",
            "obs= [ 9. 19.  0.] reward= -0.6291269703271665 done= False\n",
            "Step 43\n",
            "Action right\n",
            "obs= [ 9. 20.  0.] reward= -0.30325314723895946 done= False\n",
            "Step 44\n",
            "Action left\n",
            "obs= [ 9. 19.  0.] reward= -0.8677550046621767 done= False\n",
            "Step 45\n",
            "Action right\n",
            "obs= [ 9. 20.  0.] reward= -0.0750257002369451 done= False\n",
            "Step 46\n",
            "Action right\n",
            "obs= [ 9. 21.  0.] reward= -0.19424800796657604 done= False\n",
            "Step 47\n",
            "Action up\n",
            "obs= [ 8. 21.  0.] reward= -0.06505773551608285 done= False\n",
            "Step 48\n",
            "Action right\n",
            "obs= [ 9. 21.  0.] reward= -0.6956192281904953 done= False\n",
            "Step 49\n",
            "Action down\n",
            "obs= [10. 21.  0.] reward= -0.15587118862578864 done= False\n",
            "Step 50\n",
            "Action left\n",
            "obs= [10. 20.  0.] reward= -0.9196280992411242 done= False\n",
            "Step 51\n",
            "Action right\n",
            "obs= [10. 21.  0.] reward= -0.39246404451579653 done= False\n",
            "Step 52\n",
            "Action right\n",
            "obs= [10. 22.  0.] reward= -0.9259490588931574 done= False\n",
            "Step 53\n",
            "Action up\n",
            "obs= [ 9. 22.  0.] reward= -0.32547392428755184 done= False\n",
            "Step 54\n",
            "Action down\n",
            "obs= [10. 22.  0.] reward= -0.8706183769580226 done= False\n",
            "Step 55\n",
            "Action right\n",
            "obs= [10. 23.  0.] reward= -0.06790041897382226 done= False\n",
            "Step 56\n",
            "Action down\n",
            "obs= [11. 23.  0.] reward= -0.5637176987744383 done= False\n",
            "Step 57\n",
            "Action down\n",
            "obs= [12. 23.  0.] reward= -0.5153554668121408 done= False\n",
            "Step 58\n",
            "Action up\n",
            "obs= [11. 23.  0.] reward= -0.5807553642047629 done= False\n",
            "Step 59\n",
            "Action up\n",
            "obs= [10. 23.  0.] reward= -0.222398848177497 done= False\n",
            "Step 60\n",
            "Action down\n",
            "obs= [11. 23.  0.] reward= -0.9180223998141728 done= False\n",
            "Step 61\n",
            "Action up\n",
            "obs= [10. 23.  0.] reward= -0.9163724431896613 done= False\n",
            "Step 62\n",
            "Action right\n",
            "obs= [10. 23.  0.] reward= -5.0 done= False\n",
            "Step 63\n",
            "Action down\n",
            "obs= [11. 23.  0.] reward= -0.2819143088462367 done= False\n",
            "Step 64\n",
            "Action left\n",
            "obs= [11. 22.  0.] reward= -0.7288427431970239 done= False\n",
            "Step 65\n",
            "Action up\n",
            "obs= [10. 22.  0.] reward= -0.3246531119487377 done= False\n",
            "Step 66\n",
            "Action right\n",
            "obs= [10. 23.  0.] reward= -0.9849025484997723 done= False\n",
            "Step 67\n",
            "Action up\n",
            "obs= [ 9. 23.  0.] reward= -0.27803278824401934 done= False\n",
            "Step 68\n",
            "Action up\n",
            "obs= [ 8. 23.  0.] reward= -0.13788717732418876 done= False\n",
            "Step 69\n",
            "Action left\n",
            "obs= [ 8. 22.  0.] reward= -0.40919858614910454 done= False\n",
            "Step 70\n",
            "Action right\n",
            "obs= [ 7. 22.  0.] reward= -0.4405835345905864 done= False\n",
            "Step 71\n",
            "Action right\n",
            "obs= [ 7. 23.  0.] reward= -0.9593978719551737 done= False\n",
            "Step 72\n",
            "Action right\n",
            "obs= [ 7. 24.  0.] reward= -0.24586568798650854 done= False\n",
            "Step 73\n",
            "Action up\n",
            "obs= [ 6. 24.  0.] reward= -0.4565495075518695 done= False\n",
            "Step 74\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.3450753413927776 done= False\n",
            "Step 75\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 76\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 77\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -0.8581123774629064 done= False\n",
            "Step 78\n",
            "Action left\n",
            "obs= [ 5. 24.  0.] reward= -0.22176061869304908 done= False\n",
            "Step 79\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -0.2125416678983475 done= False\n",
            "Step 80\n",
            "Action down\n",
            "obs= [ 6. 25.  0.] reward= -0.21940787086133196 done= False\n",
            "Step 81\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -5.0 done= False\n",
            "Step 82\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -5.0 done= False\n",
            "Step 83\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.4009517022158108 done= False\n",
            "Step 84\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.5482488064604942 done= False\n",
            "Step 85\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -0.5001929457056216 done= False\n",
            "Step 86\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 87\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 88\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 89\n",
            "Action left\n",
            "obs= [ 5. 24.  0.] reward= -0.24425045313942362 done= False\n",
            "Step 90\n",
            "Action left\n",
            "obs= [ 5. 23.  0.] reward= -0.2670502950666568 done= False\n",
            "Step 91\n",
            "Action right\n",
            "obs= [ 5. 24.  0.] reward= -0.7771723190581536 done= False\n",
            "Step 92\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 93\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -0.39768202752521586 done= False\n",
            "Step 94\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 95\n",
            "Action down\n",
            "obs= [ 6. 25.  0.] reward= -0.021304478069561816 done= False\n",
            "Step 96\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.7556921785608239 done= False\n",
            "Step 97\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.11695517822865054 done= False\n",
            "Step 98\n",
            "Action right\n",
            "obs= [ 7. 25.  0.] reward= -0.029840233087288448 done= False\n",
            "Step 99\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.3438282194018407 done= False\n",
            "Step 100\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -5.0 done= False\n",
            "Step 101\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.1338427190723609 done= False\n",
            "Step 102\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.7483715820490248 done= False\n",
            "Step 103\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.5164753485630879 done= False\n",
            "Step 104\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.6090807972959364 done= False\n",
            "Step 105\n",
            "Action up\n",
            "obs= [ 7. 23.  0.] reward= -0.2166371484738734 done= False\n",
            "Step 106\n",
            "Action right\n",
            "obs= [ 7. 24.  0.] reward= -0.3931785150496897 done= False\n",
            "Step 107\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.8666652675817965 done= False\n",
            "Step 108\n",
            "Action right\n",
            "obs= [ 7. 24.  0.] reward= -0.6640576983004154 done= False\n",
            "Step 109\n",
            "Action down\n",
            "obs= [ 8. 24.  0.] reward= -0.4549537903508575 done= False\n",
            "Step 110\n",
            "Action down\n",
            "obs= [ 9. 24.  0.] reward= -0.9125149710975171 done= False\n",
            "Step 111\n",
            "Action down\n",
            "obs= [ 9. 24.  0.] reward= -5.0 done= False\n",
            "Step 112\n",
            "Action down\n",
            "obs= [ 9. 25.  0.] reward= -0.6427232759304432 done= False\n",
            "Step 113\n",
            "Action right\n",
            "obs= [ 9. 24.  0.] reward= -0.7849632761980495 done= False\n",
            "Step 114\n",
            "Action left\n",
            "obs= [ 9. 23.  0.] reward= -0.40640424629698435 done= False\n",
            "Step 115\n",
            "Action right\n",
            "obs= [ 9. 24.  0.] reward= -0.02424630786363624 done= False\n",
            "Step 116\n",
            "Action up\n",
            "obs= [ 8. 24.  0.] reward= -0.25228986232920103 done= False\n",
            "Step 117\n",
            "Action left\n",
            "obs= [ 8. 23.  0.] reward= -0.22304293108899576 done= False\n",
            "Step 118\n",
            "Action up\n",
            "obs= [ 7. 23.  0.] reward= -0.43963607499618707 done= False\n",
            "Step 119\n",
            "Action up\n",
            "obs= [ 6. 23.  0.] reward= -0.08887266035830022 done= False\n",
            "Step 120\n",
            "Action up\n",
            "obs= [ 5. 23.  0.] reward= -0.8891719470846488 done= False\n",
            "Step 121\n",
            "Action left\n",
            "obs= [ 5. 22.  0.] reward= -0.030333854317085684 done= False\n",
            "Step 122\n",
            "Action left\n",
            "obs= [ 5. 21.  0.] reward= -0.0049464199452845525 done= False\n",
            "Step 123\n",
            "Action up\n",
            "obs= [ 4. 21.  0.] reward= -0.1787136620249664 done= False\n",
            "Step 124\n",
            "Action left\n",
            "obs= [ 4. 20.  0.] reward= -0.8451882664985343 done= False\n",
            "Step 125\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -0.2769862160081773 done= False\n",
            "Step 126\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -0.7037258981747379 done= False\n",
            "Step 127\n",
            "Action down\n",
            "obs= [ 4. 19.  0.] reward= -0.3602969195313508 done= False\n",
            "Step 128\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -5.0 done= False\n",
            "Step 129\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.015220072432655551 done= False\n",
            "Step 130\n",
            "Action down\n",
            "obs= [ 5. 20.  0.] reward= -0.7325025232271707 done= False\n",
            "Step 131\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.38473695004391206 done= False\n",
            "Step 132\n",
            "Action right\n",
            "obs= [ 5. 22.  0.] reward= -0.366004513968475 done= False\n",
            "Step 133\n",
            "Action right\n",
            "obs= [ 5. 23.  0.] reward= -0.6899508383297684 done= False\n",
            "Step 134\n",
            "Action up\n",
            "obs= [ 4. 23.  0.] reward= -0.700702935292312 done= False\n",
            "Step 135\n",
            "Action left\n",
            "obs= [ 4. 22.  0.] reward= -0.428676121033449 done= False\n",
            "Step 136\n",
            "Action left\n",
            "obs= [ 4. 21.  0.] reward= -0.4185164396084402 done= False\n",
            "Step 137\n",
            "Action left\n",
            "obs= [ 4. 20.  0.] reward= -0.6662402435770616 done= False\n",
            "Step 138\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -0.6720468360314324 done= False\n",
            "Step 139\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -5.0 done= False\n",
            "Step 140\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.6483610960013242 done= False\n",
            "Step 141\n",
            "Action right\n",
            "obs= [ 4. 21.  0.] reward= -0.9957254123031457 done= False\n",
            "Step 142\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.6898891130281105 done= False\n",
            "Step 143\n",
            "Action down\n",
            "obs= [ 6. 21.  0.] reward= -0.28894402708788613 done= False\n",
            "Step 144\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.5624483400704581 done= False\n",
            "Step 145\n",
            "Action up\n",
            "obs= [ 5. 20.  0.] reward= -0.7915654258060961 done= False\n",
            "Step 146\n",
            "Action left\n",
            "obs= [ 5. 19.  0.] reward= -0.42496026932206665 done= False\n",
            "Step 147\n",
            "Action down\n",
            "obs= [ 6. 19.  0.] reward= -0.6650860687789582 done= False\n",
            "Step 148\n",
            "Action down\n",
            "obs= [ 7. 19.  0.] reward= -0.15931335665646262 done= False\n",
            "Step 149\n",
            "Action left\n",
            "obs= [ 7. 18.  0.] reward= -0.8846549582871938 done= False\n",
            "Step 150\n",
            "Action right\n",
            "obs= [ 7. 19.  0.] reward= -0.9986430073414199 done= False\n",
            "Step 151\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.584070861150233 done= False\n",
            "Step 152\n",
            "Action up\n",
            "obs= [ 6. 20.  0.] reward= -0.7528159170213359 done= False\n",
            "Step 153\n",
            "Action up\n",
            "obs= [ 5. 20.  0.] reward= -0.40896884452300697 done= False\n",
            "Step 154\n",
            "Action down\n",
            "obs= [ 6. 20.  0.] reward= -0.10559941141926088 done= False\n",
            "Step 155\n",
            "Action right\n",
            "obs= [ 6. 21.  0.] reward= -0.20529351625013426 done= False\n",
            "Step 156\n",
            "Action right\n",
            "obs= [ 6. 22.  0.] reward= -0.8390523507559999 done= False\n",
            "Step 157\n",
            "Action left\n",
            "obs= [ 6. 21.  0.] reward= -0.13888299602522847 done= False\n",
            "Step 158\n",
            "Action down\n",
            "obs= [ 7. 21.  0.] reward= -0.14949903957509036 done= False\n",
            "Step 159\n",
            "Action right\n",
            "obs= [ 7. 22.  0.] reward= -0.8457707197874016 done= False\n",
            "Step 160\n",
            "Action left\n",
            "obs= [ 7. 21.  0.] reward= -0.8255748387741381 done= False\n",
            "Step 161\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.9883175007109082 done= False\n",
            "Step 162\n",
            "Action left\n",
            "obs= [ 8. 20.  0.] reward= -0.5757636524362746 done= False\n",
            "Step 163\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.9509066416103406 done= False\n",
            "Step 164\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.486203273968856 done= False\n",
            "Step 165\n",
            "Action up\n",
            "obs= [ 6. 19.  0.] reward= -0.7540846159410403 done= False\n",
            "Step 166\n",
            "Action left\n",
            "obs= [ 6. 18.  0.] reward= -0.4743187961144768 done= False\n",
            "Step 167\n",
            "Action left\n",
            "obs= [ 5. 18.  0.] reward= -0.7489876589266339 done= False\n",
            "Step 168\n",
            "Action down\n",
            "obs= [ 6. 18.  0.] reward= -0.6108586485317082 done= False\n",
            "Step 169\n",
            "Action down\n",
            "obs= [ 7. 18.  0.] reward= -0.3704250332993254 done= False\n",
            "Step 170\n",
            "Action up\n",
            "obs= [ 6. 18.  0.] reward= -0.11095273497669222 done= False\n",
            "Step 171\n",
            "Action right\n",
            "obs= [ 6. 19.  0.] reward= -0.9550219011107081 done= False\n",
            "Step 172\n",
            "Action down\n",
            "obs= [ 7. 19.  0.] reward= -0.01411451891749349 done= False\n",
            "Step 173\n",
            "Action up\n",
            "obs= [ 6. 19.  0.] reward= -0.8965650594268728 done= False\n",
            "Step 174\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.21970642718052213 done= False\n",
            "Step 175\n",
            "Action right\n",
            "obs= [ 6. 21.  0.] reward= -0.37773655879297974 done= False\n",
            "Step 176\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.03225453407261647 done= False\n",
            "Step 177\n",
            "Action left\n",
            "obs= [ 6. 19.  0.] reward= -0.8595235836155224 done= False\n",
            "Step 178\n",
            "Action left\n",
            "obs= [ 6. 18.  0.] reward= -0.7020254628548802 done= False\n",
            "Step 179\n",
            "Action down\n",
            "obs= [ 7. 18.  0.] reward= -0.967473605954877 done= False\n",
            "Step 180\n",
            "Action right\n",
            "obs= [ 7. 19.  0.] reward= -0.5327044982574095 done= False\n",
            "Step 181\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.3240973652366186 done= False\n",
            "Step 182\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.6515563760932765 done= False\n",
            "Step 183\n",
            "Action right\n",
            "obs= [ 7. 22.  0.] reward= -0.8187285544178226 done= False\n",
            "Step 184\n",
            "Action left\n",
            "obs= [ 7. 21.  0.] reward= -0.7402112261816722 done= False\n",
            "Step 185\n",
            "Action left\n",
            "obs= [ 7. 20.  0.] reward= -0.10735603589443532 done= False\n",
            "Step 186\n",
            "Action down\n",
            "obs= [ 8. 20.  0.] reward= -0.3188655417876499 done= False\n",
            "Step 187\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.30211745506323984 done= False\n",
            "Step 188\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.8476047803781924 done= False\n",
            "Step 189\n",
            "Action down\n",
            "obs= [ 8. 19.  0.] reward= -0.6994147786172585 done= False\n",
            "Step 190\n",
            "Action down\n",
            "obs= [ 9. 19.  0.] reward= -0.6098287987160489 done= False\n",
            "Step 191\n",
            "Action up\n",
            "obs= [ 8. 19.  0.] reward= -0.8717932185518482 done= False\n",
            "Step 192\n",
            "Action up\n",
            "obs= [ 9. 19.  0.] reward= -0.2327262296441578 done= False\n",
            "Step 193\n",
            "Action right\n",
            "obs= [ 9. 20.  0.] reward= -0.07749853182893818 done= False\n",
            "Step 194\n",
            "Action left\n",
            "obs= [ 9. 19.  0.] reward= -0.19397979878999583 done= False\n",
            "Step 195\n",
            "Action up\n",
            "obs= [ 8. 19.  0.] reward= -0.3852146674140008 done= False\n",
            "Step 196\n",
            "Action down\n",
            "obs= [ 7. 19.  0.] reward= -0.4024341392174009 done= False\n",
            "Step 197\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.16184003449990492 done= False\n",
            "Step 198\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.4355582474073084 done= False\n",
            "Step 199\n",
            "Action left\n",
            "obs= [ 6. 19.  0.] reward= -0.27445814278906266 done= False\n",
            "Step 200\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.1389946736006029 done= False\n",
            "Step 201\n",
            "Action down\n",
            "obs= [ 7. 20.  0.] reward= -0.012639474746586332 done= False\n",
            "Step 202\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.527470340611568 done= False\n",
            "Step 203\n",
            "Action up\n",
            "obs= [ 6. 21.  0.] reward= -0.28312975258749984 done= False\n",
            "Step 204\n",
            "Action up\n",
            "obs= [ 5. 21.  0.] reward= -0.030108257068740762 done= False\n",
            "Step 205\n",
            "Action down\n",
            "obs= [ 6. 21.  0.] reward= -0.2435915484101896 done= False\n",
            "Step 206\n",
            "Action right\n",
            "obs= [ 6. 22.  0.] reward= -0.31099058373722843 done= False\n",
            "Step 207\n",
            "Action up\n",
            "obs= [ 5. 22.  0.] reward= -0.31851597752819083 done= False\n",
            "Step 208\n",
            "Action right\n",
            "obs= [ 5. 23.  0.] reward= -0.1870202113874878 done= False\n",
            "Step 209\n",
            "Action right\n",
            "obs= [ 5. 24.  0.] reward= -0.9844140126047436 done= False\n",
            "Step 210\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.1785175870090252 done= False\n",
            "Step 211\n",
            "Action left\n",
            "obs= [ 6. 23.  0.] reward= -0.7363661132547691 done= False\n",
            "Step 212\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.46805903581881725 done= False\n",
            "Step 213\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.046094529814235785 done= False\n",
            "Step 214\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.25209628741911283 done= False\n",
            "Step 215\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.604784077922938 done= False\n",
            "Step 216\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -0.16602223755049828 done= False\n",
            "Step 217\n",
            "Action right\n",
            "obs= [ 5. 24.  0.] reward= 1000.0 done= False\n",
            "Step 218\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.36573973616669897 done= False\n",
            "Step 219\n",
            "Action down\n",
            "obs= [ 7. 24.  0.] reward= -0.8247823383651391 done= False\n",
            "Step 220\n",
            "Action right\n",
            "obs= [ 7. 25.  0.] reward= -0.4979823369908254 done= False\n",
            "Step 221\n",
            "Action right\n",
            "obs= [ 7. 26.  0.] reward= -0.26711381587946514 done= False\n",
            "Step 222\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.47076021829105563 done= False\n",
            "Step 223\n",
            "Action down\n",
            "obs= [ 8. 25.  0.] reward= -0.8968168516924404 done= False\n",
            "Step 224\n",
            "Action down\n",
            "obs= [ 8. 24.  0.] reward= -0.5660652972414905 done= False\n",
            "Step 225\n",
            "Action left\n",
            "obs= [ 8. 23.  0.] reward= -0.992281054027396 done= False\n",
            "Step 226\n",
            "Action down\n",
            "obs= [ 9. 23.  0.] reward= -0.27473824246973755 done= False\n",
            "Step 227\n",
            "Action right\n",
            "obs= [ 9. 24.  0.] reward= -0.8147032240632668 done= False\n",
            "Step 228\n",
            "Action left\n",
            "obs= [ 9. 23.  0.] reward= -0.37363936197366476 done= False\n",
            "Step 229\n",
            "Action left\n",
            "obs= [ 9. 24.  0.] reward= -0.9475042397588531 done= False\n",
            "Step 230\n",
            "Action up\n",
            "obs= [ 8. 24.  0.] reward= -0.14919381654140074 done= False\n",
            "Step 231\n",
            "Action up\n",
            "obs= [ 7. 24.  0.] reward= -0.4172452675229438 done= False\n",
            "Step 232\n",
            "Action right\n",
            "obs= [ 7. 25.  0.] reward= -0.5709211980002387 done= False\n",
            "Step 233\n",
            "Action down\n",
            "obs= [ 8. 25.  0.] reward= -0.8390344141493586 done= False\n",
            "Step 234\n",
            "Action right\n",
            "obs= [ 8. 26.  0.] reward= -0.2875465848262946 done= False\n",
            "Step 235\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.6628958587107382 done= False\n",
            "Step 236\n",
            "Action right\n",
            "obs= [ 8. 26.  0.] reward= -0.5313555171423654 done= False\n",
            "Step 237\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.007532218411587022 done= False\n",
            "Step 238\n",
            "Action up\n",
            "obs= [ 7. 25.  0.] reward= -0.5794541396081441 done= False\n",
            "Step 239\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.8970970062032319 done= False\n",
            "Step 240\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.9170141044049974 done= False\n",
            "Step 241\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.6794614718062418 done= False\n",
            "Step 242\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -0.3068119461112807 done= False\n",
            "Step 243\n",
            "Action down\n",
            "obs= [ 6. 25.  0.] reward= -0.95049386023376 done= False\n",
            "Step 244\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.91551124906774 done= False\n",
            "Step 245\n",
            "Action right\n",
            "obs= [ 7. 26.  0.] reward= -0.40612787961418306 done= False\n",
            "Step 246\n",
            "Action up\n",
            "obs= [ 7. 26.  0.] reward= -5.0 done= False\n",
            "Step 247\n",
            "Action right\n",
            "obs= [ 7. 27.  0.] reward= -0.5452819933965068 done= False\n",
            "Step 248\n",
            "Action left\n",
            "obs= [ 7. 26.  0.] reward= -0.055854265625560395 done= False\n",
            "Step 249\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.7037458998014398 done= False\n",
            "Step 250\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.6098791175598985 done= False\n",
            "Step 251\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -0.24370937915969937 done= False\n",
            "Step 252\n",
            "Action down\n",
            "obs= [ 6. 25.  0.] reward= -0.15380101076649977 done= False\n",
            "Step 253\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.5365560135580852 done= False\n",
            "Step 254\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -0.009789885367157858 done= False\n",
            "Step 255\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -5.0 done= False\n",
            "Step 256\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.819766225983006 done= False\n",
            "Step 257\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.9146947585052266 done= False\n",
            "Step 258\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.6756772545093255 done= False\n",
            "Step 259\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.6804773657335598 done= False\n",
            "Step 260\n",
            "Action left\n",
            "obs= [ 6. 24.  0.] reward= -0.9869551575531135 done= False\n",
            "Step 261\n",
            "Action down\n",
            "obs= [ 7. 24.  0.] reward= -0.9876634403409899 done= False\n",
            "Step 262\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.11453376436478524 done= False\n",
            "Step 263\n",
            "Action up\n",
            "obs= [ 6. 23.  0.] reward= -0.8966747664462688 done= False\n",
            "Step 264\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.5980848579859191 done= False\n",
            "Step 265\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.7151343094911804 done= False\n",
            "Step 266\n",
            "Action left\n",
            "obs= [ 5. 23.  0.] reward= -0.9994745838534921 done= False\n",
            "Step 267\n",
            "Action down\n",
            "obs= [ 6. 23.  0.] reward= -0.34981088117404846 done= False\n",
            "Step 268\n",
            "Action left\n",
            "obs= [ 6. 22.  0.] reward= -0.29807883908633037 done= False\n",
            "Step 269\n",
            "Action left\n",
            "obs= [ 6. 23.  0.] reward= -0.023262635670900367 done= False\n",
            "Step 270\n",
            "Action up\n",
            "obs= [ 5. 23.  0.] reward= -0.7839083165068629 done= False\n",
            "Step 271\n",
            "Action left\n",
            "obs= [ 5. 22.  0.] reward= -0.3607320713120096 done= False\n",
            "Step 272\n",
            "Action down\n",
            "obs= [ 6. 22.  0.] reward= -0.3356921896036752 done= False\n",
            "Step 273\n",
            "Action right\n",
            "obs= [ 6. 23.  0.] reward= -0.7041729810361184 done= False\n",
            "Step 274\n",
            "Action up\n",
            "obs= [ 6. 22.  0.] reward= -0.6630231709962929 done= False\n",
            "Step 275\n",
            "Action down\n",
            "obs= [ 5. 22.  0.] reward= -0.4297221547952059 done= False\n",
            "Step 276\n",
            "Action down\n",
            "obs= [ 6. 22.  0.] reward= -0.5944451675718996 done= False\n",
            "Step 277\n",
            "Action right\n",
            "obs= [ 6. 23.  0.] reward= -0.017709166698672174 done= False\n",
            "Step 278\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.15289445904189436 done= False\n",
            "Step 279\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.1687894885453307 done= False\n",
            "Step 280\n",
            "Action left\n",
            "obs= [ 5. 23.  0.] reward= -0.19083745955096 done= False\n",
            "Step 281\n",
            "Action up\n",
            "obs= [ 4. 23.  0.] reward= -0.4269136783378775 done= False\n",
            "Step 282\n",
            "Action down\n",
            "obs= [ 5. 23.  0.] reward= -0.4592710389564897 done= False\n",
            "Step 283\n",
            "Action right\n",
            "obs= [ 5. 24.  0.] reward= -0.2760625428903385 done= False\n",
            "Step 284\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.49544603884932803 done= False\n",
            "Step 285\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -0.5522054167257081 done= False\n",
            "Step 286\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -0.6895364965166813 done= False\n",
            "Step 287\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 288\n",
            "Action left\n",
            "obs= [ 5. 24.  0.] reward= -0.6919772131176242 done= False\n",
            "Step 289\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 290\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 291\n",
            "Action left\n",
            "obs= [ 5. 23.  0.] reward= -0.3734758358261432 done= False\n",
            "Step 292\n",
            "Action down\n",
            "obs= [ 6. 23.  0.] reward= -0.9944377585634215 done= False\n",
            "Step 293\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.08003114590389193 done= False\n",
            "Step 294\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -0.27024027559835284 done= False\n",
            "Step 295\n",
            "Action down\n",
            "obs= [ 7. 25.  0.] reward= -0.8458294020452678 done= False\n",
            "Step 296\n",
            "Action right\n",
            "obs= [ 7. 26.  0.] reward= -0.7823567574520068 done= False\n",
            "Step 297\n",
            "Action down\n",
            "obs= [ 8. 26.  0.] reward= -0.19518671954386835 done= False\n",
            "Step 298\n",
            "Action up\n",
            "obs= [ 7. 26.  0.] reward= -0.8012500804520696 done= False\n",
            "Step 299\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.3841577354743855 done= False\n",
            "Step 300\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.9651454136113607 done= False\n",
            "Step 301\n",
            "Action right\n",
            "obs= [ 7. 25.  0.] reward= -0.14311657340998118 done= False\n",
            "Step 302\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.14217256936843836 done= False\n",
            "Step 303\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.45069657375164796 done= False\n",
            "Step 304\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.3730375429810918 done= False\n",
            "Step 305\n",
            "Action down\n",
            "obs= [ 9. 23.  0.] reward= -0.7989575032968959 done= False\n",
            "Step 306\n",
            "Action right\n",
            "obs= [ 9. 24.  0.] reward= -0.42310859003792833 done= False\n",
            "Step 307\n",
            "Action down\n",
            "obs= [ 9. 24.  0.] reward= -5.0 done= False\n",
            "Step 308\n",
            "Action right\n",
            "obs= [ 9. 25.  0.] reward= -0.4182226442190484 done= False\n",
            "Step 309\n",
            "Action down\n",
            "obs= [10. 25.  0.] reward= -0.07438923860541946 done= False\n",
            "Step 310\n",
            "Action up\n",
            "obs= [11. 25.  0.] reward= -0.9896699110069452 done= False\n",
            "Step 311\n",
            "Action left\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 312\n",
            "Action left\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 313\n",
            "Action left\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 314\n",
            "Action up\n",
            "obs= [10. 25.  0.] reward= -0.5829925737957123 done= False\n",
            "Step 315\n",
            "Action down\n",
            "obs= [11. 25.  0.] reward= -0.31950110491611206 done= False\n",
            "Step 316\n",
            "Action right\n",
            "obs= [10. 25.  0.] reward= -0.6776388181452583 done= False\n",
            "Step 317\n",
            "Action down\n",
            "obs= [11. 25.  0.] reward= -0.3911444355045627 done= False\n",
            "Step 318\n",
            "Action right\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 319\n",
            "Action down\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 320\n",
            "Action up\n",
            "obs= [10. 25.  0.] reward= -0.07694990855819117 done= False\n",
            "Step 321\n",
            "Action down\n",
            "obs= [11. 25.  0.] reward= -0.014471390590141575 done= False\n",
            "Step 322\n",
            "Action left\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 323\n",
            "Action down\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 324\n",
            "Action left\n",
            "obs= [11. 25.  0.] reward= -5.0 done= False\n",
            "Step 325\n",
            "Action up\n",
            "obs= [10. 25.  0.] reward= -0.015184592591086066 done= False\n",
            "Step 326\n",
            "Action up\n",
            "obs= [ 9. 25.  0.] reward= -0.3961437024345724 done= False\n",
            "Step 327\n",
            "Action right\n",
            "obs= [ 9. 26.  0.] reward= -0.3104972774207265 done= False\n",
            "Step 328\n",
            "Action up\n",
            "obs= [ 8. 26.  0.] reward= -0.9178914690278258 done= False\n",
            "Step 329\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.49607336502616084 done= False\n",
            "Step 330\n",
            "Action up\n",
            "obs= [ 7. 25.  0.] reward= -0.8174741801412184 done= False\n",
            "Step 331\n",
            "Action up\n",
            "obs= [ 6. 25.  0.] reward= -0.17446617662441755 done= False\n",
            "Step 332\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -0.510850706963648 done= False\n",
            "Step 333\n",
            "Action up\n",
            "obs= [ 5. 25.  0.] reward= -5.0 done= False\n",
            "Step 334\n",
            "Action left\n",
            "obs= [ 5. 24.  0.] reward= -0.245969654578309 done= False\n",
            "Step 335\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -5.0 done= False\n",
            "Step 336\n",
            "Action right\n",
            "obs= [ 5. 25.  0.] reward= -0.657772623278815 done= False\n",
            "Step 337\n",
            "Action left\n",
            "obs= [ 5. 24.  0.] reward= -0.16674098061238074 done= False\n",
            "Step 338\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.7567547477615973 done= False\n",
            "Step 339\n",
            "Action right\n",
            "obs= [ 6. 25.  0.] reward= -0.2138998227780754 done= False\n",
            "Step 340\n",
            "Action down\n",
            "obs= [ 6. 24.  0.] reward= -0.19244778998530132 done= False\n",
            "Step 341\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.512707864444653 done= False\n",
            "Step 342\n",
            "Action right\n",
            "obs= [ 7. 25.  0.] reward= -0.644276205715077 done= False\n",
            "Step 343\n",
            "Action down\n",
            "obs= [ 8. 25.  0.] reward= -0.9218817709955077 done= False\n",
            "Step 344\n",
            "Action left\n",
            "obs= [ 8. 24.  0.] reward= -0.1668830348227004 done= False\n",
            "Step 345\n",
            "Action up\n",
            "obs= [ 8. 25.  0.] reward= -0.9924718456092363 done= False\n",
            "Step 346\n",
            "Action left\n",
            "obs= [ 8. 24.  0.] reward= -0.07906999715674223 done= False\n",
            "Step 347\n",
            "Action right\n",
            "obs= [ 8. 25.  0.] reward= -0.943938830277824 done= False\n",
            "Step 348\n",
            "Action down\n",
            "obs= [ 9. 25.  0.] reward= -0.2088209825443491 done= False\n",
            "Step 349\n",
            "Action up\n",
            "obs= [ 8. 25.  0.] reward= -0.3802112003073751 done= False\n",
            "Step 350\n",
            "Action left\n",
            "obs= [ 8. 24.  0.] reward= -0.08296776924104543 done= False\n",
            "Step 351\n",
            "Action right\n",
            "obs= [ 8. 25.  0.] reward= -0.03253064859189969 done= False\n",
            "Step 352\n",
            "Action right\n",
            "obs= [ 8. 26.  0.] reward= -0.27528265593538137 done= False\n",
            "Step 353\n",
            "Action down\n",
            "obs= [ 9. 26.  0.] reward= -0.2612434122109599 done= False\n",
            "Step 354\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.25804772351441607 done= False\n",
            "Step 355\n",
            "Action up\n",
            "obs= [ 8. 25.  0.] reward= -0.2807421181558031 done= False\n",
            "Step 356\n",
            "Action right\n",
            "obs= [ 8. 26.  0.] reward= -0.23940123455629325 done= False\n",
            "Step 357\n",
            "Action right\n",
            "obs= [ 8. 27.  0.] reward= -0.8154508946410395 done= False\n",
            "Step 358\n",
            "Action up\n",
            "obs= [ 8. 26.  0.] reward= -0.4993374188683325 done= False\n",
            "Step 359\n",
            "Action up\n",
            "obs= [ 7. 26.  0.] reward= -0.2963111755941026 done= False\n",
            "Step 360\n",
            "Action down\n",
            "obs= [ 8. 26.  0.] reward= -0.8508366988160491 done= False\n",
            "Step 361\n",
            "Action up\n",
            "obs= [ 7. 26.  0.] reward= -0.07606303808118653 done= False\n",
            "Step 362\n",
            "Action up\n",
            "obs= [ 7. 26.  0.] reward= -5.0 done= False\n",
            "Step 363\n",
            "Action left\n",
            "obs= [ 8. 26.  0.] reward= -0.3696772748393675 done= False\n",
            "Step 364\n",
            "Action down\n",
            "obs= [ 9. 26.  0.] reward= -0.46053649092124194 done= False\n",
            "Step 365\n",
            "Action right\n",
            "obs= [ 9. 27.  0.] reward= -0.007635371004912739 done= False\n",
            "Step 366\n",
            "Action right\n",
            "obs= [ 9. 28.  0.] reward= -0.1797909956011957 done= False\n",
            "Step 367\n",
            "Action left\n",
            "obs= [ 9. 28.  0.] reward= -5.0 done= False\n",
            "Step 368\n",
            "Action down\n",
            "obs= [ 9. 27.  0.] reward= -0.21844794094075437 done= False\n",
            "Step 369\n",
            "Action left\n",
            "obs= [ 9. 26.  0.] reward= -0.3609378959042625 done= False\n",
            "Step 370\n",
            "Action up\n",
            "obs= [ 8. 26.  0.] reward= -0.4065670721009895 done= False\n",
            "Step 371\n",
            "Action right\n",
            "obs= [ 8. 27.  0.] reward= -0.6117949408866529 done= False\n",
            "Step 372\n",
            "Action right\n",
            "obs= [ 8. 27.  0.] reward= -5.0 done= False\n",
            "Step 373\n",
            "Action up\n",
            "obs= [ 8. 26.  0.] reward= -0.40653491063661595 done= False\n",
            "Step 374\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.5270607084954775 done= False\n",
            "Step 375\n",
            "Action down\n",
            "obs= [ 9. 25.  0.] reward= -0.003005587262958742 done= False\n",
            "Step 376\n",
            "Action up\n",
            "obs= [ 8. 25.  0.] reward= -0.18063656402537365 done= False\n",
            "Step 377\n",
            "Action up\n",
            "obs= [ 7. 25.  0.] reward= -0.9137754723153327 done= False\n",
            "Step 378\n",
            "Action left\n",
            "obs= [ 7. 24.  0.] reward= -0.033098273408264545 done= False\n",
            "Step 379\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.24044758782244902 done= False\n",
            "Step 380\n",
            "Action left\n",
            "obs= [ 7. 22.  0.] reward= -0.807540496419945 done= False\n",
            "Step 381\n",
            "Action down\n",
            "obs= [ 8. 22.  0.] reward= -0.21906843306000834 done= False\n",
            "Step 382\n",
            "Action right\n",
            "obs= [ 8. 23.  0.] reward= -0.8881166994940899 done= False\n",
            "Step 383\n",
            "Action down\n",
            "obs= [ 9. 23.  0.] reward= -0.9963322618417078 done= False\n",
            "Step 384\n",
            "Action down\n",
            "obs= [10. 23.  0.] reward= -0.5729617930228855 done= False\n",
            "Step 385\n",
            "Action right\n",
            "obs= [10. 23.  0.] reward= -5.0 done= False\n",
            "Step 386\n",
            "Action right\n",
            "obs= [10. 23.  0.] reward= -5.0 done= False\n",
            "Step 387\n",
            "Action down\n",
            "obs= [11. 23.  0.] reward= -0.4172754528706991 done= False\n",
            "Step 388\n",
            "Action up\n",
            "obs= [10. 23.  0.] reward= -0.8840527779817572 done= False\n",
            "Step 389\n",
            "Action left\n",
            "obs= [10. 22.  0.] reward= -0.16916923892646407 done= False\n",
            "Step 390\n",
            "Action up\n",
            "obs= [ 9. 22.  0.] reward= -0.8263544809679325 done= False\n",
            "Step 391\n",
            "Action up\n",
            "obs= [ 8. 22.  0.] reward= -0.20100169156828085 done= False\n",
            "Step 392\n",
            "Action up\n",
            "obs= [ 7. 22.  0.] reward= -0.5537368989562592 done= False\n",
            "Step 393\n",
            "Action up\n",
            "obs= [ 6. 22.  0.] reward= -0.8096347906615762 done= False\n",
            "Step 394\n",
            "Action up\n",
            "obs= [ 5. 22.  0.] reward= -0.4257578501558139 done= False\n",
            "Step 395\n",
            "Action left\n",
            "obs= [ 5. 21.  0.] reward= -0.5734204486945838 done= False\n",
            "Step 396\n",
            "Action right\n",
            "obs= [ 5. 22.  0.] reward= -0.24855041560321034 done= False\n",
            "Step 397\n",
            "Action up\n",
            "obs= [ 4. 22.  0.] reward= -0.9761869980946783 done= False\n",
            "Step 398\n",
            "Action left\n",
            "obs= [ 3. 22.  0.] reward= -0.9283374785029309 done= False\n",
            "Step 399\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -0.563511678198917 done= False\n",
            "Step 400\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 401\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 402\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 403\n",
            "Action left\n",
            "obs= [ 3. 22.  0.] reward= -0.03999142020725699 done= False\n",
            "Step 404\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.6640297701010625 done= False\n",
            "Step 405\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -0.8390651482407184 done= False\n",
            "Step 406\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.4338432720374228 done= False\n",
            "Step 407\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -0.6646848579568742 done= False\n",
            "Step 408\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 409\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -0.984630240786709 done= False\n",
            "Step 410\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 411\n",
            "Action left\n",
            "obs= [ 3. 22.  0.] reward= -0.9324195064812208 done= False\n",
            "Step 412\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.20563983061286373 done= False\n",
            "Step 413\n",
            "Action left\n",
            "obs= [ 4. 23.  0.] reward= -0.5183248156252035 done= False\n",
            "Step 414\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -0.3895423276551573 done= False\n",
            "Step 415\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 416\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 417\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 418\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 419\n",
            "Action up\n",
            "obs= [ 4. 23.  0.] reward= -0.6019537601825641 done= False\n",
            "Step 420\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -0.025805757811824925 done= False\n",
            "Step 421\n",
            "Action left\n",
            "obs= [ 3. 22.  0.] reward= -0.3025889227378773 done= False\n",
            "Step 422\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.6525416607929244 done= False\n",
            "Step 423\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.665978990194198 done= False\n",
            "Step 424\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -0.09228473243428248 done= False\n",
            "Step 425\n",
            "Action down\n",
            "obs= [ 4. 19.  0.] reward= -0.06403387478490341 done= False\n",
            "Step 426\n",
            "Action right\n",
            "obs= [ 3. 19.  0.] reward= -0.7793910775281474 done= False\n",
            "Step 427\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -5.0 done= False\n",
            "Step 428\n",
            "Action right\n",
            "obs= [ 4. 19.  0.] reward= -0.803278799861655 done= False\n",
            "Step 429\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.4933910562698207 done= False\n",
            "Step 430\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -0.9327062171616384 done= False\n",
            "Step 431\n",
            "Action down\n",
            "obs= [ 5. 19.  0.] reward= -0.38138014082953775 done= False\n",
            "Step 432\n",
            "Action left\n",
            "obs= [ 5. 18.  0.] reward= -0.13319275256654772 done= False\n",
            "Step 433\n",
            "Action down\n",
            "obs= [ 6. 18.  0.] reward= -0.47033846097569076 done= False\n",
            "Step 434\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -0.431044566582924 done= False\n",
            "Step 435\n",
            "Action right\n",
            "obs= [ 5. 19.  0.] reward= -0.6312661626811271 done= False\n",
            "Step 436\n",
            "Action up\n",
            "obs= [ 4. 19.  0.] reward= -0.8617864609494481 done= False\n",
            "Step 437\n",
            "Action down\n",
            "obs= [ 5. 19.  0.] reward= -0.10439120682545999 done= False\n",
            "Step 438\n",
            "Action up\n",
            "obs= [ 4. 19.  0.] reward= -0.7923565481088374 done= False\n",
            "Step 439\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -5.0 done= False\n",
            "Step 440\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -0.8166442532653213 done= False\n",
            "Step 441\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -5.0 done= False\n",
            "Step 442\n",
            "Action down\n",
            "obs= [ 3. 19.  0.] reward= -5.0 done= False\n",
            "Step 443\n",
            "Action right\n",
            "obs= [ 3. 20.  0.] reward= -0.4752265643160829 done= False\n",
            "Step 444\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -0.652189871885587 done= False\n",
            "Step 445\n",
            "Action right\n",
            "obs= [ 3. 20.  0.] reward= -0.5993004346199495 done= False\n",
            "Step 446\n",
            "Action down\n",
            "obs= [ 4. 20.  0.] reward= -0.7951384050443764 done= False\n",
            "Step 447\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -0.5290405432038426 done= False\n",
            "Step 448\n",
            "Action down\n",
            "obs= [ 4. 20.  0.] reward= -0.5423081702414076 done= False\n",
            "Step 449\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -0.5660445436712465 done= False\n",
            "Step 450\n",
            "Action down\n",
            "obs= [ 4. 20.  0.] reward= -0.7283308623433669 done= False\n",
            "Step 451\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -0.021980585893448645 done= False\n",
            "Step 452\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.7370171040317439 done= False\n",
            "Step 453\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.9416633840607663 done= False\n",
            "Step 454\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -0.11078857780028073 done= False\n",
            "Step 455\n",
            "Action down\n",
            "obs= [ 4. 19.  0.] reward= -0.12445034325426063 done= False\n",
            "Step 456\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.4284725987254626 done= False\n",
            "Step 457\n",
            "Action down\n",
            "obs= [ 5. 20.  0.] reward= -0.35639099666127205 done= False\n",
            "Step 458\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.20016400562703007 done= False\n",
            "Step 459\n",
            "Action up\n",
            "obs= [ 4. 21.  0.] reward= -0.8914711646647822 done= False\n",
            "Step 460\n",
            "Action right\n",
            "obs= [ 4. 22.  0.] reward= -0.8133624649628429 done= False\n",
            "Step 461\n",
            "Action right\n",
            "obs= [ 4. 23.  0.] reward= -0.7042880528639366 done= False\n",
            "Step 462\n",
            "Action down\n",
            "obs= [ 5. 23.  0.] reward= -0.9655362666275802 done= False\n",
            "Step 463\n",
            "Action down\n",
            "obs= [ 6. 23.  0.] reward= -0.907432839413307 done= False\n",
            "Step 464\n",
            "Action left\n",
            "obs= [ 6. 22.  0.] reward= -0.7499998680628588 done= False\n",
            "Step 465\n",
            "Action up\n",
            "obs= [ 5. 22.  0.] reward= -0.035880713139002096 done= False\n",
            "Step 466\n",
            "Action left\n",
            "obs= [ 5. 21.  0.] reward= -0.32895212341073776 done= False\n",
            "Step 467\n",
            "Action down\n",
            "obs= [ 6. 21.  0.] reward= -0.1979677932509566 done= False\n",
            "Step 468\n",
            "Action right\n",
            "obs= [ 6. 22.  0.] reward= -0.19666201063663247 done= False\n",
            "Step 469\n",
            "Action right\n",
            "obs= [ 6. 23.  0.] reward= -0.49697732142686324 done= False\n",
            "Step 470\n",
            "Action down\n",
            "obs= [ 7. 23.  0.] reward= -0.1581464114012483 done= False\n",
            "Step 471\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.047207856140662474 done= False\n",
            "Step 472\n",
            "Action down\n",
            "obs= [ 9. 23.  0.] reward= -0.6912699088414673 done= False\n",
            "Step 473\n",
            "Action left\n",
            "obs= [ 9. 22.  0.] reward= -0.08682206956867089 done= False\n",
            "Step 474\n",
            "Action up\n",
            "obs= [ 8. 22.  0.] reward= -0.7478930682473763 done= False\n",
            "Step 475\n",
            "Action down\n",
            "obs= [ 9. 22.  0.] reward= -0.24689781545094713 done= False\n",
            "Step 476\n",
            "Action left\n",
            "obs= [ 9. 21.  0.] reward= -0.4750593318968598 done= False\n",
            "Step 477\n",
            "Action right\n",
            "obs= [ 9. 22.  0.] reward= -0.7012364561138238 done= False\n",
            "Step 478\n",
            "Action right\n",
            "obs= [ 9. 23.  0.] reward= -0.4463362629712676 done= False\n",
            "Step 479\n",
            "Action left\n",
            "obs= [ 9. 22.  0.] reward= -0.16809707751468284 done= False\n",
            "Step 480\n",
            "Action up\n",
            "obs= [ 8. 22.  0.] reward= -0.42095493823738084 done= False\n",
            "Step 481\n",
            "Action left\n",
            "obs= [ 8. 21.  0.] reward= -0.8941023828327203 done= False\n",
            "Step 482\n",
            "Action left\n",
            "obs= [ 8. 20.  0.] reward= -0.6195873448796263 done= False\n",
            "Step 483\n",
            "Action down\n",
            "obs= [ 9. 20.  0.] reward= -0.5987140691806054 done= False\n",
            "Step 484\n",
            "Action up\n",
            "obs= [ 8. 20.  0.] reward= -0.8877483109382244 done= False\n",
            "Step 485\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.32623317058748635 done= False\n",
            "Step 486\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.2470373423514367 done= False\n",
            "Step 487\n",
            "Action up\n",
            "obs= [ 7. 22.  0.] reward= -0.0331260104430795 done= False\n",
            "Step 488\n",
            "Action right\n",
            "obs= [ 7. 23.  0.] reward= -0.747074201163998 done= False\n",
            "Step 489\n",
            "Action left\n",
            "obs= [ 7. 22.  0.] reward= -0.8826578547144144 done= False\n",
            "Step 490\n",
            "Action left\n",
            "obs= [ 7. 23.  0.] reward= -0.8872411446271969 done= False\n",
            "Step 491\n",
            "Action up\n",
            "obs= [ 6. 23.  0.] reward= -0.8264520234892481 done= False\n",
            "Step 492\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.26686160367753975 done= False\n",
            "Step 493\n",
            "Action left\n",
            "obs= [ 6. 23.  0.] reward= -0.47254807252475683 done= False\n",
            "Step 494\n",
            "Action right\n",
            "obs= [ 6. 24.  0.] reward= -0.12569218669471016 done= False\n",
            "Step 495\n",
            "Action up\n",
            "obs= [ 5. 24.  0.] reward= -0.08156863009799364 done= False\n",
            "Step 496\n",
            "Action left\n",
            "obs= [ 5. 23.  0.] reward= -0.3837928484354661 done= False\n",
            "Step 497\n",
            "Action down\n",
            "obs= [ 6. 23.  0.] reward= -0.8097059986231354 done= False\n",
            "Step 498\n",
            "Action left\n",
            "obs= [ 6. 22.  0.] reward= -0.46481940285897105 done= False\n",
            "Step 499\n",
            "Action left\n",
            "obs= [ 6. 21.  0.] reward= -0.9558410632500663 done= False\n",
            "Step 500\n",
            "Action down\n",
            "obs= [ 7. 21.  0.] reward= -0.12655723920567996 done= False\n",
            "Step 501\n",
            "Action right\n",
            "obs= [ 7. 22.  0.] reward= -0.6163345137224544 done= False\n",
            "Step 502\n",
            "Action left\n",
            "obs= [ 7. 21.  0.] reward= -0.36611913315918154 done= False\n",
            "Step 503\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.36046008660789275 done= False\n",
            "Step 504\n",
            "Action up\n",
            "obs= [ 7. 21.  0.] reward= -0.9322546277094121 done= False\n",
            "Step 505\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.6212176584632255 done= False\n",
            "Step 506\n",
            "Action left\n",
            "obs= [ 7. 21.  0.] reward= -0.608844088927441 done= False\n",
            "Step 507\n",
            "Action up\n",
            "obs= [ 6. 21.  0.] reward= -0.08976369967182751 done= False\n",
            "Step 508\n",
            "Action left\n",
            "obs= [ 6. 20.  0.] reward= -0.3486163051316097 done= False\n",
            "Step 509\n",
            "Action up\n",
            "obs= [ 5. 20.  0.] reward= -0.9138706310689462 done= False\n",
            "Step 510\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.6203213109602992 done= False\n",
            "Step 511\n",
            "Action up\n",
            "obs= [ 4. 21.  0.] reward= -0.5946289997273781 done= False\n",
            "Step 512\n",
            "Action up\n",
            "obs= [ 3. 21.  0.] reward= -0.30984211318399146 done= False\n",
            "Step 513\n",
            "Action down\n",
            "obs= [ 4. 21.  0.] reward= -0.49784869326511383 done= False\n",
            "Step 514\n",
            "Action up\n",
            "obs= [ 3. 21.  0.] reward= -0.4417062681494166 done= False\n",
            "Step 515\n",
            "Action right\n",
            "obs= [ 3. 22.  0.] reward= -0.1498672396250308 done= False\n",
            "Step 516\n",
            "Action left\n",
            "obs= [ 3. 23.  0.] reward= -0.20005072006517455 done= False\n",
            "Step 517\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 518\n",
            "Action down\n",
            "obs= [ 4. 23.  0.] reward= -0.7327127780628165 done= False\n",
            "Step 519\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -0.7365387327388313 done= False\n",
            "Step 520\n",
            "Action left\n",
            "obs= [ 3. 22.  0.] reward= -0.8351075811056066 done= False\n",
            "Step 521\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.09755792929700557 done= False\n",
            "Step 522\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.6160901380394347 done= False\n",
            "Step 523\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -0.19445612217547992 done= False\n",
            "Step 524\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.1442290658735802 done= False\n",
            "Step 525\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -0.48312264941472927 done= False\n",
            "Step 526\n",
            "Action right\n",
            "obs= [ 3. 22.  0.] reward= -0.4554880760839479 done= False\n",
            "Step 527\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.16668905462336492 done= False\n",
            "Step 528\n",
            "Action right\n",
            "obs= [ 4. 23.  0.] reward= -0.8130829177856264 done= False\n",
            "Step 529\n",
            "Action right\n",
            "obs= [ 4. 23.  0.] reward= -5.0 done= False\n",
            "Step 530\n",
            "Action down\n",
            "obs= [ 5. 23.  0.] reward= -0.9343123050536187 done= False\n",
            "Step 531\n",
            "Action up\n",
            "obs= [ 4. 23.  0.] reward= -0.9720446995033701 done= False\n",
            "Step 532\n",
            "Action up\n",
            "obs= [ 3. 23.  0.] reward= -0.2356953253163374 done= False\n",
            "Step 533\n",
            "Action right\n",
            "obs= [ 3. 23.  0.] reward= -5.0 done= False\n",
            "Step 534\n",
            "Action right\n",
            "obs= [ 3. 22.  0.] reward= -0.1056229052887252 done= False\n",
            "Step 535\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.4435306435758791 done= False\n",
            "Step 536\n",
            "Action down\n",
            "obs= [ 4. 21.  0.] reward= -0.05688965653395206 done= False\n",
            "Step 537\n",
            "Action up\n",
            "obs= [ 3. 21.  0.] reward= -0.7049987515255652 done= False\n",
            "Step 538\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -5.0 done= False\n",
            "Step 539\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.19520497541332738 done= False\n",
            "Step 540\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -0.4236959050246294 done= False\n",
            "Step 541\n",
            "Action right\n",
            "obs= [ 3. 20.  0.] reward= -0.8481285317470555 done= False\n",
            "Step 542\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -5.0 done= False\n",
            "Step 543\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -5.0 done= False\n",
            "Step 544\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -0.859933230261352 done= False\n",
            "Step 545\n",
            "Action right\n",
            "obs= [ 3. 22.  0.] reward= -0.07359344884678387 done= False\n",
            "Step 546\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 547\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 548\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 549\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.22029506240848296 done= False\n",
            "Step 550\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -0.27188708262050587 done= False\n",
            "Step 551\n",
            "Action down\n",
            "obs= [ 4. 22.  0.] reward= -0.018812205441788188 done= False\n",
            "Step 552\n",
            "Action left\n",
            "obs= [ 4. 21.  0.] reward= -0.6906493576675913 done= False\n",
            "Step 553\n",
            "Action right\n",
            "obs= [ 4. 22.  0.] reward= -0.22029820466087613 done= False\n",
            "Step 554\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -0.8011805063040067 done= False\n",
            "Step 555\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.12922019685994512 done= False\n",
            "Step 556\n",
            "Action up\n",
            "obs= [ 3. 21.  0.] reward= -5.0 done= False\n",
            "Step 557\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.33605559186989686 done= False\n",
            "Step 558\n",
            "Action down\n",
            "obs= [ 4. 20.  0.] reward= -0.009396941875665199 done= False\n",
            "Step 559\n",
            "Action down\n",
            "obs= [ 5. 20.  0.] reward= -0.7807107868841805 done= False\n",
            "Step 560\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.6796406433299058 done= False\n",
            "Step 561\n",
            "Action up\n",
            "obs= [ 4. 21.  0.] reward= -0.19985564760221886 done= False\n",
            "Step 562\n",
            "Action down\n",
            "obs= [ 5. 21.  0.] reward= -0.8305461470639222 done= False\n",
            "Step 563\n",
            "Action left\n",
            "obs= [ 5. 20.  0.] reward= -0.26293189853602283 done= False\n",
            "Step 564\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.16804914931663495 done= False\n",
            "Step 565\n",
            "Action right\n",
            "obs= [ 5. 22.  0.] reward= -0.8808464466821389 done= False\n",
            "Step 566\n",
            "Action right\n",
            "obs= [ 5. 23.  0.] reward= -0.7920799684010866 done= False\n",
            "Step 567\n",
            "Action left\n",
            "obs= [ 5. 22.  0.] reward= -0.732298849185993 done= False\n",
            "Step 568\n",
            "Action down\n",
            "obs= [ 6. 22.  0.] reward= -0.8073871421419665 done= False\n",
            "Step 569\n",
            "Action up\n",
            "obs= [ 5. 22.  0.] reward= -0.49238337145639643 done= False\n",
            "Step 570\n",
            "Action up\n",
            "obs= [ 4. 22.  0.] reward= -0.946221073997346 done= False\n",
            "Step 571\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -0.3512629124027046 done= False\n",
            "Step 572\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 573\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 574\n",
            "Action left\n",
            "obs= [ 3. 21.  0.] reward= -0.42551698214824574 done= False\n",
            "Step 575\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.940942968573195 done= False\n",
            "Step 576\n",
            "Action left\n",
            "obs= [ 3. 19.  0.] reward= -0.9923215714494539 done= False\n",
            "Step 577\n",
            "Action right\n",
            "obs= [ 3. 20.  0.] reward= -0.8156239703778043 done= False\n",
            "Step 578\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -5.0 done= False\n",
            "Step 579\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -5.0 done= False\n",
            "Step 580\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -0.9780735407126886 done= False\n",
            "Step 581\n",
            "Action right\n",
            "obs= [ 3. 22.  0.] reward= -0.9272523625067608 done= False\n",
            "Step 582\n",
            "Action up\n",
            "obs= [ 3. 22.  0.] reward= -5.0 done= False\n",
            "Step 583\n",
            "Action right\n",
            "obs= [ 4. 22.  0.] reward= -0.19425291268182399 done= False\n",
            "Step 584\n",
            "Action left\n",
            "obs= [ 4. 21.  0.] reward= -0.16673996206025699 done= False\n",
            "Step 585\n",
            "Action right\n",
            "obs= [ 4. 22.  0.] reward= -0.2652570679532408 done= False\n",
            "Step 586\n",
            "Action right\n",
            "obs= [ 4. 23.  0.] reward= -0.35187422839964355 done= False\n",
            "Step 587\n",
            "Action right\n",
            "obs= [ 4. 23.  0.] reward= -5.0 done= False\n",
            "Step 588\n",
            "Action down\n",
            "obs= [ 5. 23.  0.] reward= -0.8412983144758389 done= False\n",
            "Step 589\n",
            "Action up\n",
            "obs= [ 4. 23.  0.] reward= -0.0019220966029438458 done= False\n",
            "Step 590\n",
            "Action down\n",
            "obs= [ 5. 23.  0.] reward= -0.4222383527705801 done= False\n",
            "Step 591\n",
            "Action left\n",
            "obs= [ 5. 22.  0.] reward= -0.3566374657208614 done= False\n",
            "Step 592\n",
            "Action up\n",
            "obs= [ 4. 22.  0.] reward= -0.835611485385877 done= False\n",
            "Step 593\n",
            "Action down\n",
            "obs= [ 5. 22.  0.] reward= -0.5575877334996839 done= False\n",
            "Step 594\n",
            "Action right\n",
            "obs= [ 5. 21.  0.] reward= -0.27579876009111626 done= False\n",
            "Step 595\n",
            "Action up\n",
            "obs= [ 4. 21.  0.] reward= -0.31687473579398073 done= False\n",
            "Step 596\n",
            "Action right\n",
            "obs= [ 3. 21.  0.] reward= -0.4090760594516456 done= False\n",
            "Step 597\n",
            "Action left\n",
            "obs= [ 3. 20.  0.] reward= -0.8995311386871562 done= False\n",
            "Step 598\n",
            "Action down\n",
            "obs= [ 4. 20.  0.] reward= -0.20545704703458567 done= False\n",
            "Step 599\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -0.8207433900721426 done= False\n",
            "Step 600\n",
            "Action up\n",
            "obs= [ 3. 20.  0.] reward= -5.0 done= False\n",
            "Step 601\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.546686856369668 done= False\n",
            "Step 602\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -0.9056949359665714 done= False\n",
            "Step 603\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -0.9036773556622715 done= False\n",
            "Step 604\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -5.0 done= False\n",
            "Step 605\n",
            "Action up\n",
            "obs= [ 3. 19.  0.] reward= -5.0 done= False\n",
            "Step 606\n",
            "Action down\n",
            "obs= [ 4. 19.  0.] reward= -0.312825008211175 done= False\n",
            "Step 607\n",
            "Action down\n",
            "obs= [ 5. 19.  0.] reward= -0.381042526654784 done= False\n",
            "Step 608\n",
            "Action up\n",
            "obs= [ 4. 19.  0.] reward= -0.4093027513606733 done= False\n",
            "Step 609\n",
            "Action right\n",
            "obs= [ 4. 20.  0.] reward= -0.18967993666221494 done= False\n",
            "Step 610\n",
            "Action left\n",
            "obs= [ 4. 19.  0.] reward= -0.8647326759826861 done= False\n",
            "Step 611\n",
            "Action down\n",
            "obs= [ 5. 19.  0.] reward= -0.7184844829582876 done= False\n",
            "Step 612\n",
            "Action left\n",
            "obs= [ 5. 18.  0.] reward= -0.13743758852114496 done= False\n",
            "Step 613\n",
            "Action right\n",
            "obs= [ 5. 19.  0.] reward= -0.35399219915231095 done= False\n",
            "Step 614\n",
            "Action down\n",
            "obs= [ 6. 19.  0.] reward= -0.9013105423736553 done= False\n",
            "Step 615\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.7969249189450217 done= False\n",
            "Step 616\n",
            "Action down\n",
            "obs= [ 7. 20.  0.] reward= -0.8908556355088926 done= False\n",
            "Step 617\n",
            "Action left\n",
            "obs= [ 6. 20.  0.] reward= -0.8227729203247465 done= False\n",
            "Step 618\n",
            "Action left\n",
            "obs= [ 6. 19.  0.] reward= -0.006647543135723244 done= False\n",
            "Step 619\n",
            "Action left\n",
            "obs= [ 6. 18.  0.] reward= -0.12055814587621305 done= False\n",
            "Step 620\n",
            "Action down\n",
            "obs= [ 7. 18.  0.] reward= -0.8954645350490426 done= False\n",
            "Step 621\n",
            "Action down\n",
            "obs= [ 8. 18.  0.] reward= -0.7146382609736303 done= False\n",
            "Step 622\n",
            "Action right\n",
            "obs= [ 8. 19.  0.] reward= -0.9426619908371561 done= False\n",
            "Step 623\n",
            "Action left\n",
            "obs= [ 8. 18.  0.] reward= -0.6655798925508271 done= False\n",
            "Step 624\n",
            "Action down\n",
            "obs= [ 9. 18.  0.] reward= -0.7003780598520642 done= False\n",
            "Step 625\n",
            "Action left\n",
            "obs= [ 9. 17.  0.] reward= -0.40477152020121077 done= False\n",
            "Step 626\n",
            "Action up\n",
            "obs= [ 8. 17.  0.] reward= -0.3089290240276088 done= False\n",
            "Step 627\n",
            "Action right\n",
            "obs= [ 8. 18.  0.] reward= -0.7437986701956384 done= False\n",
            "Step 628\n",
            "Action up\n",
            "obs= [ 7. 18.  0.] reward= -0.9731743057321625 done= False\n",
            "Step 629\n",
            "Action down\n",
            "obs= [ 8. 18.  0.] reward= -0.6437640027952138 done= False\n",
            "Step 630\n",
            "Action right\n",
            "obs= [ 8. 19.  0.] reward= -0.8808930531131386 done= False\n",
            "Step 631\n",
            "Action up\n",
            "obs= [ 7. 19.  0.] reward= -0.1196353710774467 done= False\n",
            "Step 632\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.47583040038615887 done= False\n",
            "Step 633\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.3509023163637318 done= False\n",
            "Step 634\n",
            "Action down\n",
            "obs= [ 8. 19.  0.] reward= -0.3050728658524169 done= False\n",
            "Step 635\n",
            "Action right\n",
            "obs= [ 8. 20.  0.] reward= -0.30869099993384763 done= False\n",
            "Step 636\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.14456802711958527 done= False\n",
            "Step 637\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.5716868045777738 done= False\n",
            "Step 638\n",
            "Action up\n",
            "obs= [ 6. 21.  0.] reward= -0.29139184256224604 done= False\n",
            "Step 639\n",
            "Action up\n",
            "obs= [ 5. 21.  0.] reward= -0.300736087449909 done= False\n",
            "Step 640\n",
            "Action up\n",
            "obs= [ 6. 21.  0.] reward= -0.1427617921429878 done= False\n",
            "Step 641\n",
            "Action up\n",
            "obs= [ 6. 22.  0.] reward= -0.8981374399765906 done= False\n",
            "Step 642\n",
            "Action left\n",
            "obs= [ 6. 21.  0.] reward= -0.5350370603205115 done= False\n",
            "Step 643\n",
            "Action down\n",
            "obs= [ 7. 21.  0.] reward= -0.6354917687003737 done= False\n",
            "Step 644\n",
            "Action left\n",
            "obs= [ 7. 20.  0.] reward= -0.7678660683801188 done= False\n",
            "Step 645\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.5262644905478583 done= False\n",
            "Step 646\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.87037159442297 done= False\n",
            "Step 647\n",
            "Action down\n",
            "obs= [ 9. 21.  0.] reward= -0.5748589521544029 done= False\n",
            "Step 648\n",
            "Action left\n",
            "obs= [ 9. 20.  0.] reward= -0.014912301744449907 done= False\n",
            "Step 649\n",
            "Action down\n",
            "obs= [10. 20.  0.] reward= -0.5580457282175403 done= False\n",
            "Step 650\n",
            "Action down\n",
            "obs= [11. 20.  0.] reward= -0.41117338195860187 done= False\n",
            "Step 651\n",
            "Action right\n",
            "obs= [11. 21.  0.] reward= -0.6171490473444387 done= False\n",
            "Step 652\n",
            "Action up\n",
            "obs= [10. 21.  0.] reward= -0.7297071299535207 done= False\n",
            "Step 653\n",
            "Action right\n",
            "obs= [10. 22.  0.] reward= -0.054129469691660326 done= False\n",
            "Step 654\n",
            "Action left\n",
            "obs= [10. 21.  0.] reward= -0.5770706951854109 done= False\n",
            "Step 655\n",
            "Action right\n",
            "obs= [10. 22.  0.] reward= -0.5147752437445062 done= False\n",
            "Step 656\n",
            "Action up\n",
            "obs= [ 9. 22.  0.] reward= -0.4233448835664807 done= False\n",
            "Step 657\n",
            "Action up\n",
            "obs= [ 8. 22.  0.] reward= -0.928583752452586 done= False\n",
            "Step 658\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.03674407788772971 done= False\n",
            "Step 659\n",
            "Action up\n",
            "obs= [ 7. 23.  0.] reward= -0.8141661424753118 done= False\n",
            "Step 660\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.207508843193713 done= False\n",
            "Step 661\n",
            "Action up\n",
            "obs= [ 7. 23.  0.] reward= -0.02407829912784365 done= False\n",
            "Step 662\n",
            "Action down\n",
            "obs= [ 8. 23.  0.] reward= -0.5562905663122091 done= False\n",
            "Step 663\n",
            "Action down\n",
            "obs= [ 9. 23.  0.] reward= -0.014395399680350196 done= False\n",
            "Step 664\n",
            "Action right\n",
            "obs= [ 9. 24.  0.] reward= -0.5571612538792657 done= False\n",
            "Step 665\n",
            "Action right\n",
            "obs= [ 9. 25.  0.] reward= -0.8899858044866491 done= False\n",
            "Step 666\n",
            "Action left\n",
            "obs= [ 9. 24.  0.] reward= -0.7936030182733713 done= False\n",
            "Step 667\n",
            "Action left\n",
            "obs= [ 9. 23.  0.] reward= -0.1363620503788795 done= False\n",
            "Step 668\n",
            "Action up\n",
            "obs= [ 8. 23.  0.] reward= -0.27329376613852363 done= False\n",
            "Step 669\n",
            "Action up\n",
            "obs= [ 7. 23.  0.] reward= -0.6294854180595575 done= False\n",
            "Step 670\n",
            "Action right\n",
            "obs= [ 7. 24.  0.] reward= -0.046350180945964325 done= False\n",
            "Step 671\n",
            "Action up\n",
            "obs= [ 6. 24.  0.] reward= -0.41486100404909565 done= False\n",
            "Step 672\n",
            "Action left\n",
            "obs= [ 6. 23.  0.] reward= -0.8543240496175559 done= False\n",
            "Step 673\n",
            "Action left\n",
            "obs= [ 6. 22.  0.] reward= -0.1973459342832713 done= False\n",
            "Step 674\n",
            "Action left\n",
            "obs= [ 6. 21.  0.] reward= -0.42410896283645616 done= False\n",
            "Step 675\n",
            "Action up\n",
            "obs= [ 5. 21.  0.] reward= -0.321779536293862 done= False\n",
            "Step 676\n",
            "Action left\n",
            "obs= [ 5. 20.  0.] reward= -0.5585202675134895 done= False\n",
            "Step 677\n",
            "Action down\n",
            "obs= [ 6. 20.  0.] reward= -0.12713288077723028 done= False\n",
            "Step 678\n",
            "Action up\n",
            "obs= [ 5. 20.  0.] reward= -0.5378461472753993 done= False\n",
            "Step 679\n",
            "Action up\n",
            "obs= [ 6. 20.  0.] reward= -0.4921993303283988 done= False\n",
            "Step 680\n",
            "Action down\n",
            "obs= [ 7. 20.  0.] reward= -0.687862621785159 done= False\n",
            "Step 681\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.19445775141456612 done= False\n",
            "Step 682\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.2462144293876889 done= False\n",
            "Step 683\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.02751967004283906 done= False\n",
            "Step 684\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.8273692538264175 done= False\n",
            "Step 685\n",
            "Action down\n",
            "obs= [ 8. 20.  0.] reward= -0.391316679917058 done= False\n",
            "Step 686\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.26836321201027735 done= False\n",
            "Step 687\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.879042762862928 done= False\n",
            "Step 688\n",
            "Action left\n",
            "obs= [ 7. 20.  0.] reward= -0.04454326669000119 done= False\n",
            "Step 689\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.6142226565427609 done= False\n",
            "Step 690\n",
            "Action left\n",
            "obs= [ 7. 18.  0.] reward= -0.19324064670855023 done= False\n",
            "Step 691\n",
            "Action left\n",
            "obs= [ 7. 17.  0.] reward= -0.022963791889647367 done= False\n",
            "Step 692\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -0.47484302150519864 done= False\n",
            "Step 693\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -0.2446827340913893 done= False\n",
            "Step 694\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.21375341516682778 done= False\n",
            "Step 695\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.5151700712941688 done= False\n",
            "Step 696\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.9658940876367129 done= False\n",
            "Step 697\n",
            "Action right\n",
            "obs= [ 6. 18.  0.] reward= -0.08309493337070062 done= False\n",
            "Step 698\n",
            "Action down\n",
            "obs= [ 7. 18.  0.] reward= -0.6536718778941094 done= False\n",
            "Step 699\n",
            "Action up\n",
            "obs= [ 6. 18.  0.] reward= -0.36684063743974515 done= False\n",
            "Step 700\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -0.16397554124119684 done= False\n",
            "Step 701\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.6304133427778491 done= False\n",
            "Step 702\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.8368000323096177 done= False\n",
            "Step 703\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -5.0 done= False\n",
            "Step 704\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.9929253274519615 done= False\n",
            "Step 705\n",
            "Action left\n",
            "obs= [ 5. 16.  0.] reward= -0.14535127582560792 done= False\n",
            "Step 706\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.1270388224029667 done= False\n",
            "Step 707\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 708\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.9573380505595654 done= False\n",
            "Step 709\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 710\n",
            "Action down\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 711\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.4697853177577853 done= False\n",
            "Step 712\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 713\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.9693384961117835 done= False\n",
            "Step 714\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.07773379129291791 done= False\n",
            "Step 715\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.21931437873950765 done= False\n",
            "Step 716\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 717\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 718\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 719\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -0.5089953554544833 done= False\n",
            "Step 720\n",
            "Action left\n",
            "obs= [ 3. 14.  0.] reward= -0.2400926094457021 done= False\n",
            "Step 721\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 722\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 723\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -0.056599338357725126 done= False\n",
            "Step 724\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 725\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 726\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 727\n",
            "Action right\n",
            "obs= [ 3. 14.  0.] reward= -0.04689896281085526 done= False\n",
            "Step 728\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 729\n",
            "Action down\n",
            "obs= [ 3. 15.  0.] reward= -0.8563491248262786 done= False\n",
            "Step 730\n",
            "Action down\n",
            "obs= [ 4. 15.  0.] reward= -0.17495361008389143 done= False\n",
            "Step 731\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 732\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.18953523931198746 done= False\n",
            "Step 733\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -0.4690087339369978 done= False\n",
            "Step 734\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 735\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 736\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.08045855669304969 done= False\n",
            "Step 737\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 738\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 739\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.6319370837553652 done= False\n",
            "Step 740\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 741\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 742\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -0.21306078262323003 done= False\n",
            "Step 743\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -0.23090258889323212 done= False\n",
            "Step 744\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -0.5240021721104703 done= False\n",
            "Step 745\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 746\n",
            "Action left\n",
            "obs= [ 2. 13.  0.] reward= -0.6606704873332412 done= False\n",
            "Step 747\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -0.39212617507090153 done= False\n",
            "Step 748\n",
            "Action down\n",
            "obs= [ 2. 13.  0.] reward= -0.799583510773233 done= False\n",
            "Step 749\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -0.8085197997160368 done= False\n",
            "Step 750\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 751\n",
            "Action right\n",
            "obs= [ 3. 14.  0.] reward= -0.9345620219883041 done= False\n",
            "Step 752\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 753\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -0.43976450378961207 done= False\n",
            "Step 754\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 755\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 756\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 757\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 758\n",
            "Action left\n",
            "obs= [ 3. 14.  0.] reward= -0.09534780455522263 done= False\n",
            "Step 759\n",
            "Action right\n",
            "obs= [ 3. 15.  0.] reward= -0.6721449498660808 done= False\n",
            "Step 760\n",
            "Action down\n",
            "obs= [ 4. 15.  0.] reward= -0.20136124970527325 done= False\n",
            "Step 761\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 762\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 763\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 764\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 765\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 766\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -0.27661213426896736 done= False\n",
            "Step 767\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -5.0 done= False\n",
            "Step 768\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -5.0 done= False\n",
            "Step 769\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -5.0 done= False\n",
            "Step 770\n",
            "Action right\n",
            "obs= [ 3. 15.  0.] reward= -5.0 done= False\n",
            "Step 771\n",
            "Action down\n",
            "obs= [ 3. 15.  0.] reward= -5.0 done= False\n",
            "Step 772\n",
            "Action down\n",
            "obs= [ 4. 15.  0.] reward= -0.39726823653533594 done= False\n",
            "Step 773\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.23545580763499774 done= False\n",
            "Step 774\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.839116973582171 done= False\n",
            "Step 775\n",
            "Action up\n",
            "obs= [ 3. 15.  0.] reward= -0.8563642282542231 done= False\n",
            "Step 776\n",
            "Action left\n",
            "obs= [ 3. 14.  0.] reward= -0.17992758576365575 done= False\n",
            "Step 777\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -0.012408861976235364 done= False\n",
            "Step 778\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 779\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 780\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 781\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 782\n",
            "Action up\n",
            "obs= [ 2. 13.  0.] reward= -0.2923955056951295 done= False\n",
            "Step 783\n",
            "Action up\n",
            "obs= [ 1. 13.  0.] reward= -0.7053872325021936 done= False\n",
            "Step 784\n",
            "Action up\n",
            "obs= [ 2. 13.  0.] reward= -0.4232116778716539 done= False\n",
            "Step 785\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -0.9227484824830869 done= False\n",
            "Step 786\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 787\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 788\n",
            "Action right\n",
            "obs= [ 3. 14.  0.] reward= -0.4656544004296168 done= False\n",
            "Step 789\n",
            "Action up\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 790\n",
            "Action left\n",
            "obs= [ 3. 13.  0.] reward= -0.42422407782700533 done= False\n",
            "Step 791\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 792\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 793\n",
            "Action up\n",
            "obs= [ 2. 13.  0.] reward= -0.7040829378140008 done= False\n",
            "Step 794\n",
            "Action up\n",
            "obs= [ 1. 13.  0.] reward= -0.45402509532616786 done= False\n",
            "Step 795\n",
            "Action down\n",
            "obs= [ 1. 13.  0.] reward= -5.0 done= False\n",
            "Step 796\n",
            "Action down\n",
            "obs= [ 2. 13.  0.] reward= -0.7290597821595817 done= False\n",
            "Step 797\n",
            "Action right\n",
            "obs= [ 2. 13.  0.] reward= -5.0 done= False\n",
            "Step 798\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -0.3688199614891309 done= False\n",
            "Step 799\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -5.0 done= False\n",
            "Step 800\n",
            "Action up\n",
            "obs= [ 2. 13.  0.] reward= -0.9111058568106202 done= False\n",
            "Step 801\n",
            "Action down\n",
            "obs= [ 3. 13.  0.] reward= -0.5324653502228897 done= False\n",
            "Step 802\n",
            "Action right\n",
            "obs= [ 3. 14.  0.] reward= -0.18552233658090644 done= False\n",
            "Step 803\n",
            "Action up\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 804\n",
            "Action up\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 805\n",
            "Action up\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 806\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 807\n",
            "Action up\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 808\n",
            "Action down\n",
            "obs= [ 3. 14.  0.] reward= -5.0 done= False\n",
            "Step 809\n",
            "Action right\n",
            "obs= [ 3. 15.  0.] reward= -0.27584068205984813 done= False\n",
            "Step 810\n",
            "Action down\n",
            "obs= [ 4. 15.  0.] reward= -0.5950980140651033 done= False\n",
            "Step 811\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.5722843327318355 done= False\n",
            "Step 812\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -0.42059340458940253 done= False\n",
            "Step 813\n",
            "Action right\n",
            "obs= [ 5. 17.  0.] reward= -0.7597827877038147 done= False\n",
            "Step 814\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -5.0 done= False\n",
            "Step 815\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.5112764164280644 done= False\n",
            "Step 816\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 817\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 818\n",
            "Action right\n",
            "obs= [ 6. 18.  0.] reward= -0.12331103875608107 done= False\n",
            "Step 819\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -0.8254898957884883 done= False\n",
            "Step 820\n",
            "Action right\n",
            "obs= [ 5. 19.  0.] reward= -0.7422326050628233 done= False\n",
            "Step 821\n",
            "Action down\n",
            "obs= [ 6. 19.  0.] reward= -0.07084521099522001 done= False\n",
            "Step 822\n",
            "Action right\n",
            "obs= [ 6. 20.  0.] reward= -0.6803736571356201 done= False\n",
            "Step 823\n",
            "Action down\n",
            "obs= [ 7. 20.  0.] reward= -0.5694761406427726 done= False\n",
            "Step 824\n",
            "Action down\n",
            "obs= [ 8. 20.  0.] reward= -0.46052325284705575 done= False\n",
            "Step 825\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.6824965167483343 done= False\n",
            "Step 826\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.14328920733975237 done= False\n",
            "Step 827\n",
            "Action left\n",
            "obs= [ 7. 18.  0.] reward= -0.7678047598974936 done= False\n",
            "Step 828\n",
            "Action right\n",
            "obs= [ 7. 19.  0.] reward= -0.9694231742576952 done= False\n",
            "Step 829\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.6412770756238841 done= False\n",
            "Step 830\n",
            "Action left\n",
            "obs= [ 7. 19.  0.] reward= -0.8177399367541228 done= False\n",
            "Step 831\n",
            "Action right\n",
            "obs= [ 7. 20.  0.] reward= -0.9689356462426001 done= False\n",
            "Step 832\n",
            "Action right\n",
            "obs= [ 7. 21.  0.] reward= -0.9134097597743249 done= False\n",
            "Step 833\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.7959213622676209 done= False\n",
            "Step 834\n",
            "Action left\n",
            "obs= [ 8. 20.  0.] reward= -0.8706448894229817 done= False\n",
            "Step 835\n",
            "Action up\n",
            "obs= [ 7. 20.  0.] reward= -0.5465053716057665 done= False\n",
            "Step 836\n",
            "Action up\n",
            "obs= [ 7. 21.  0.] reward= -0.05972499161445921 done= False\n",
            "Step 837\n",
            "Action down\n",
            "obs= [ 8. 21.  0.] reward= -0.96026586744429 done= False\n",
            "Step 838\n",
            "Action down\n",
            "obs= [ 9. 21.  0.] reward= -0.14714370551813327 done= False\n",
            "Step 839\n",
            "Action left\n",
            "obs= [ 9. 20.  0.] reward= -0.8144033106330107 done= False\n",
            "Step 840\n",
            "Action down\n",
            "obs= [ 8. 20.  0.] reward= -0.41025987195673763 done= False\n",
            "Step 841\n",
            "Action down\n",
            "obs= [ 9. 20.  0.] reward= -0.6828452497479274 done= False\n",
            "Step 842\n",
            "Action up\n",
            "obs= [ 9. 19.  0.] reward= -0.9034356330955684 done= False\n",
            "Step 843\n",
            "Action right\n",
            "obs= [10. 19.  0.] reward= -0.7919059157964746 done= False\n",
            "Step 844\n",
            "Action up\n",
            "obs= [ 9. 19.  0.] reward= -0.48650475812162286 done= False\n",
            "Step 845\n",
            "Action down\n",
            "obs= [10. 19.  0.] reward= -0.19331282565018826 done= False\n",
            "Step 846\n",
            "Action left\n",
            "obs= [10. 18.  0.] reward= -0.1785089948966948 done= False\n",
            "Step 847\n",
            "Action left\n",
            "obs= [10. 17.  0.] reward= -0.265532982490582 done= False\n",
            "Step 848\n",
            "Action up\n",
            "obs= [ 9. 17.  0.] reward= -0.050830573977199145 done= False\n",
            "Step 849\n",
            "Action left\n",
            "obs= [ 9. 16.  0.] reward= -0.44274312581912934 done= False\n",
            "Step 850\n",
            "Action up\n",
            "obs= [ 9. 16.  0.] reward= -5.0 done= False\n",
            "Step 851\n",
            "Action up\n",
            "obs= [ 9. 16.  0.] reward= -5.0 done= False\n",
            "Step 852\n",
            "Action left\n",
            "obs= [ 9. 15.  0.] reward= -0.6000991921796418 done= False\n",
            "Step 853\n",
            "Action up\n",
            "obs= [ 9. 15.  0.] reward= -5.0 done= False\n",
            "Step 854\n",
            "Action left\n",
            "obs= [ 9. 15.  0.] reward= -5.0 done= False\n",
            "Step 855\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.11752556490920785 done= False\n",
            "Step 856\n",
            "Action up\n",
            "obs= [ 9. 16.  0.] reward= -5.0 done= False\n",
            "Step 857\n",
            "Action left\n",
            "obs= [ 9. 15.  0.] reward= -0.8662813371526784 done= False\n",
            "Step 858\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.06577077045473989 done= False\n",
            "Step 859\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.8030829863474661 done= False\n",
            "Step 860\n",
            "Action left\n",
            "obs= [ 9. 16.  0.] reward= -0.01036278293994719 done= False\n",
            "Step 861\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.9149096800549456 done= False\n",
            "Step 862\n",
            "Action up\n",
            "obs= [ 8. 17.  0.] reward= -0.8254141401968605 done= False\n",
            "Step 863\n",
            "Action up\n",
            "obs= [ 7. 17.  0.] reward= -0.1370885612693865 done= False\n",
            "Step 864\n",
            "Action left\n",
            "obs= [ 7. 17.  0.] reward= -5.0 done= False\n",
            "Step 865\n",
            "Action left\n",
            "obs= [ 7. 17.  0.] reward= -5.0 done= False\n",
            "Step 866\n",
            "Action left\n",
            "obs= [ 7. 17.  0.] reward= -5.0 done= False\n",
            "Step 867\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -0.4402260144748843 done= False\n",
            "Step 868\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 869\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -0.7270000216639532 done= False\n",
            "Step 870\n",
            "Action left\n",
            "obs= [ 5. 16.  0.] reward= -0.5296918365768634 done= False\n",
            "Step 871\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 872\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.6106928560740318 done= False\n",
            "Step 873\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.10335850310437955 done= False\n",
            "Step 874\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.46438563154985635 done= False\n",
            "Step 875\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -0.90873199359803 done= False\n",
            "Step 876\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.7656532374537435 done= False\n",
            "Step 877\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 878\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -0.28707462340115675 done= False\n",
            "Step 879\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 880\n",
            "Action down\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 881\n",
            "Action down\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 882\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.9560938578088697 done= False\n",
            "Step 883\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 884\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 885\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 886\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -0.3180029596631694 done= False\n",
            "Step 887\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -0.07451847986680715 done= False\n",
            "Step 888\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 889\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 890\n",
            "Action up\n",
            "obs= [ 4. 15.  0.] reward= -0.4620990055091935 done= False\n",
            "Step 891\n",
            "Action right\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 892\n",
            "Action left\n",
            "obs= [ 4. 15.  0.] reward= -5.0 done= False\n",
            "Step 893\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -0.15618464814957822 done= False\n",
            "Step 894\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 895\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 896\n",
            "Action left\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 897\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 898\n",
            "Action down\n",
            "obs= [ 5. 15.  0.] reward= -5.0 done= False\n",
            "Step 899\n",
            "Action right\n",
            "obs= [ 5. 16.  0.] reward= -0.4251768322516487 done= False\n",
            "Step 900\n",
            "Action down\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 901\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 902\n",
            "Action down\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 903\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 904\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 905\n",
            "Action up\n",
            "obs= [ 5. 16.  0.] reward= -5.0 done= False\n",
            "Step 906\n",
            "Action right\n",
            "obs= [ 5. 17.  0.] reward= -0.029703975825847495 done= False\n",
            "Step 907\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.43924851648771013 done= False\n",
            "Step 908\n",
            "Action down\n",
            "obs= [ 7. 17.  0.] reward= -0.8182879700754252 done= False\n",
            "Step 909\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -0.13626108502890266 done= False\n",
            "Step 910\n",
            "Action down\n",
            "obs= [ 7. 17.  0.] reward= -0.25749457793962893 done= False\n",
            "Step 911\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -0.1665876360486349 done= False\n",
            "Step 912\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 913\n",
            "Action down\n",
            "obs= [ 7. 17.  0.] reward= -0.9113257147374599 done= False\n",
            "Step 914\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -0.6243958009998388 done= False\n",
            "Step 915\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 916\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -0.4335861843783335 done= False\n",
            "Step 917\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.9489305897105195 done= False\n",
            "Step 918\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -5.0 done= False\n",
            "Step 919\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.2569307938675096 done= False\n",
            "Step 920\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.7582561349663678 done= False\n",
            "Step 921\n",
            "Action down\n",
            "obs= [ 6. 18.  0.] reward= -0.8642514407551259 done= False\n",
            "Step 922\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -0.19087116271013838 done= False\n",
            "Step 923\n",
            "Action up\n",
            "obs= [ 5. 17.  0.] reward= -0.16396488407285448 done= False\n",
            "Step 924\n",
            "Action right\n",
            "obs= [ 5. 18.  0.] reward= -0.904852432367266 done= False\n",
            "Step 925\n",
            "Action up\n",
            "obs= [ 5. 18.  0.] reward= -5.0 done= False\n",
            "Step 926\n",
            "Action left\n",
            "obs= [ 5. 17.  0.] reward= -0.020140255659456097 done= False\n",
            "Step 927\n",
            "Action down\n",
            "obs= [ 6. 17.  0.] reward= -0.07283688746922046 done= False\n",
            "Step 928\n",
            "Action left\n",
            "obs= [ 6. 17.  0.] reward= -5.0 done= False\n",
            "Step 929\n",
            "Action right\n",
            "obs= [ 6. 18.  0.] reward= -0.404198274818959 done= False\n",
            "Step 930\n",
            "Action down\n",
            "obs= [ 7. 18.  0.] reward= -0.36970893391082305 done= False\n",
            "Step 931\n",
            "Action left\n",
            "obs= [ 7. 17.  0.] reward= -0.18175038074536332 done= False\n",
            "Step 932\n",
            "Action up\n",
            "obs= [ 6. 17.  0.] reward= -0.3977452179721155 done= False\n",
            "Step 933\n",
            "Action down\n",
            "obs= [ 7. 17.  0.] reward= -0.11815945361454516 done= False\n",
            "Step 934\n",
            "Action right\n",
            "obs= [ 7. 18.  0.] reward= -0.35395212030169043 done= True\n",
            "Goal reached! reward= -0.35395212030169043\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "obs, _ = env.reset()\n",
        "env.render()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "print(env.action_space.sample())\n",
        "\n",
        "actions = ['up','down','left','right']\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 3000\n",
        "state_history = []\n",
        "state_history.append(obs)\n",
        "for step in range(n_steps):\n",
        "    print(f\"Step {step + 1}\")\n",
        "    action = np.random.randint(4)\n",
        "    print(f\"Action {actions[action]}\")\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    state_history.append(obs)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    env.render()\n",
        "    if done:\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break\n",
        "\n",
        "# print(np.array(state_history))\n",
        "env.show_path(np.array(state_history))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv1e1qJETfHU"
      },
      "source": [
        "### Try it with Stable-Baselines\n",
        "\n",
        "Once your environment follow the gym interface, it is quite easy to plug in any algorithm from stable-baselines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "PQfLBE28SNDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "384fb5d9-17df-4c9e-a7b8-201ed985219c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-0d4da59cd8ed>:37: DeprecationWarning: This function is deprecated. Please call randint(0, 20 + 1) instead\n",
            "  x, y = rnd(0,shape[1]//2)*2, rnd(0,shape[0]//2)*2\n",
            "<ipython-input-108-0d4da59cd8ed>:46: DeprecationWarning: This function is deprecated. Please call randint(0, 2 + 1) instead\n",
            "  y_,x_ = neighbours[rnd(0,len(neighbours)-1)]\n",
            "<ipython-input-108-0d4da59cd8ed>:46: DeprecationWarning: This function is deprecated. Please call randint(0, 3 + 1) instead\n",
            "  y_,x_ = neighbours[rnd(0,len(neighbours)-1)]\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO, A2C, DQN, TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "vec_env = make_vec_env(MazeEnv_new, n_envs=1, env_kwargs=dict(width=41, height=41))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "zRV4Q7FVUKB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f22de36-123d-4106-9d85-ff908f6e0a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# Train the agent\n",
        "model = PPO(\"MlpPolicy\", vec_env, verbose=1, learning_rate=0.00001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.learn(total_timesteps=250000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B4uVno1Y6-B8",
        "outputId": "409f4644-7e49-426f-e714-fb7a9e241b82"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------\n",
            "| time/              |      |\n",
            "|    fps             | 1187 |\n",
            "|    iterations      | 1    |\n",
            "|    time_elapsed    | 1    |\n",
            "|    total_timesteps | 2048 |\n",
            "-----------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1e+03        |\n",
            "|    ep_rew_mean          | 416          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 836          |\n",
            "|    iterations           | 2            |\n",
            "|    time_elapsed         | 4            |\n",
            "|    total_timesteps      | 4096         |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007034128 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 1.59e-05     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 9.03e+03     |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.000622    |\n",
            "|    value_loss           | 1.24e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.01e+03      |\n",
            "|    ep_rew_mean          | -65.4         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 756           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 8             |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4496851e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.00263       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 175           |\n",
            "|    n_updates            | 1250          |\n",
            "|    policy_gradient_loss | -7.15e-05     |\n",
            "|    value_loss           | 4.67e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 977           |\n",
            "|    ep_rew_mean          | -418          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 689           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 11            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5528087e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.00202       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.2e+03       |\n",
            "|    n_updates            | 1260          |\n",
            "|    policy_gradient_loss | -0.000183     |\n",
            "|    value_loss           | 8.71e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 913           |\n",
            "|    ep_rew_mean          | -470          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 677           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 15            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1179818e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.00256       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.85e+03      |\n",
            "|    n_updates            | 1270          |\n",
            "|    policy_gradient_loss | -8.92e-05     |\n",
            "|    value_loss           | 4.45e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 913          |\n",
            "|    ep_rew_mean          | -470         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 670          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 18           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.413397e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.00262      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 4.27e+03     |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.000195    |\n",
            "|    value_loss           | 8.74e+03     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 999           |\n",
            "|    ep_rew_mean          | -111          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 667           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.4162887e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.0027        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 338           |\n",
            "|    n_updates            | 1290          |\n",
            "|    policy_gradient_loss | -3.18e-05     |\n",
            "|    value_loss           | 8.87e+03      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 966          |\n",
            "|    ep_rew_mean          | -153         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 646          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 25           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.050556e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.00149      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 9.21e+03     |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -1.84e-05    |\n",
            "|    value_loss           | 2.03e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 1.01e+03      |\n",
            "|    ep_rew_mean          | -78.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 647           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 28            |\n",
            "|    total_timesteps      | 18432         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.1846746e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -0.0024       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 4.47e+03      |\n",
            "|    n_updates            | 1310          |\n",
            "|    policy_gradient_loss | -0.000159     |\n",
            "|    value_loss           | 1.62e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.02e+03     |\n",
            "|    ep_rew_mean          | -139         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 648          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 31           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.189487e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -0.00173     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 947          |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -4.14e-05    |\n",
            "|    value_loss           | 1.18e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 924           |\n",
            "|    ep_rew_mean          | -198          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 641           |\n",
            "|    iterations           | 11            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 22528         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.9242454e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.000371      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.7e+03       |\n",
            "|    n_updates            | 1330          |\n",
            "|    policy_gradient_loss | -0.000185     |\n",
            "|    value_loss           | 8.46e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 784           |\n",
            "|    ep_rew_mean          | -176          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 636           |\n",
            "|    iterations           | 12            |\n",
            "|    time_elapsed         | 38            |\n",
            "|    total_timesteps      | 24576         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00068093394 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.0566        |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 175           |\n",
            "|    n_updates            | 1340          |\n",
            "|    policy_gradient_loss | -0.00183      |\n",
            "|    value_loss           | 314           |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 779           |\n",
            "|    ep_rew_mean          | -95.7         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 637           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 41            |\n",
            "|    total_timesteps      | 26624         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015864533 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.000647      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 3.66e+03      |\n",
            "|    n_updates            | 1350          |\n",
            "|    policy_gradient_loss | -9.61e-05     |\n",
            "|    value_loss           | 8.12e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 799           |\n",
            "|    ep_rew_mean          | -72.6         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 638           |\n",
            "|    iterations           | 14            |\n",
            "|    time_elapsed         | 44            |\n",
            "|    total_timesteps      | 28672         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7429586e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | -0.00209      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.37e+04      |\n",
            "|    n_updates            | 1360          |\n",
            "|    policy_gradient_loss | -7.24e-05     |\n",
            "|    value_loss           | 1.68e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 825           |\n",
            "|    ep_rew_mean          | -97.2         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 630           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 48            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.0750545e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -5.2e-05      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 8.65e+03      |\n",
            "|    n_updates            | 1370          |\n",
            "|    policy_gradient_loss | -0.000354     |\n",
            "|    value_loss           | 1.23e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 794          |\n",
            "|    ep_rew_mean          | -163         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 51           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009932648 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.000616     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 278          |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00143     |\n",
            "|    value_loss           | 4.11e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 794          |\n",
            "|    ep_rew_mean          | -163         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 55           |\n",
            "|    total_timesteps      | 34816        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001399095 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.00336      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 281          |\n",
            "|    n_updates            | 1390         |\n",
            "|    policy_gradient_loss | -0.000516    |\n",
            "|    value_loss           | 4.16e+03     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 794          |\n",
            "|    ep_rew_mean          | -163         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 632          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 58           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010535833 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 6.14e-06     |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 9.1e+03      |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    value_loss           | 1.18e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 900           |\n",
            "|    ep_rew_mean          | -219          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 626           |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 62            |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0674186e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.00139       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 5.25e+03      |\n",
            "|    n_updates            | 1410          |\n",
            "|    policy_gradient_loss | -0.000258     |\n",
            "|    value_loss           | 8.92e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 912           |\n",
            "|    ep_rew_mean          | -238          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 627           |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 65            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013657284 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 7.83e-05      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 7.12e+03      |\n",
            "|    n_updates            | 1420          |\n",
            "|    policy_gradient_loss | -2.93e-05     |\n",
            "|    value_loss           | 1.61e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 934           |\n",
            "|    ep_rew_mean          | -178          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 629           |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 68            |\n",
            "|    total_timesteps      | 43008         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4436304e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.00189       |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 6.25e+03      |\n",
            "|    n_updates            | 1430          |\n",
            "|    policy_gradient_loss | -9.49e-05     |\n",
            "|    value_loss           | 1.27e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 907           |\n",
            "|    ep_rew_mean          | -162          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 626           |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 71            |\n",
            "|    total_timesteps      | 45056         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6200298e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0.000724      |\n",
            "|    learning_rate        | 1e-05         |\n",
            "|    loss                 | 1.09e+03      |\n",
            "|    n_updates            | 1440          |\n",
            "|    policy_gradient_loss | -4.96e-05     |\n",
            "|    value_loss           | 1.59e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 921          |\n",
            "|    ep_rew_mean          | -187         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 625          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 75           |\n",
            "|    total_timesteps      | 47104        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.949274e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0.00201      |\n",
            "|    learning_rate        | 1e-05        |\n",
            "|    loss                 | 2.62e+03     |\n",
            "|    n_updates            | 1450         |\n",
            "|    policy_gradient_loss | -0.000314    |\n",
            "|    value_loss           | 8.37e+03     |\n",
            "------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-147-1c84db0d43ca>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 315\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    316\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;31m# Convert to pytorch tensor or to TensorDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0mobs_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_as_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                 \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    650\u001b[0m         \u001b[0;31m# Evaluate the values for the given observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_vf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;31m# Here mean_actions are the logits before the softmax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmean_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMultiCategoricalDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;31m# Here mean_actions are the flattened logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/distributions.py\u001b[0m in \u001b[0;36mproba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mproba_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_logits\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSelfCategoricalDistribution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         )\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                     raise ValueError(\n\u001b[1;32m     69\u001b[0m                         \u001b[0;34mf\"Expected parameter {param} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "BJbeiF0RUN-p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "outputId": "aeecb28f-c01d-45c7-a7e2-ff3b18a06eaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action right\n",
            "obs= [10.  8.  0.] reward= -0.8199043067104413 done= False\n",
            "Step 2\n",
            "Action right\n",
            "obs= [10.  9.  0.] reward= -0.5550956711480965 done= False\n",
            "Step 3\n",
            "Action right\n",
            "obs= [10. 10.  0.] reward= -0.5694476488764697 done= False\n",
            "Step 4\n",
            "Action right\n",
            "obs= [10.  9.  0.] reward= -0.9997200786653847 done= False\n",
            "Step 5\n",
            "Action right\n",
            "obs= [10. 10.  0.] reward= -0.1363314460146532 done= False\n",
            "Step 6\n",
            "Action right\n",
            "obs= [10. 11.  0.] reward= -0.2376704858096137 done= False\n",
            "Step 7\n",
            "Action right\n",
            "obs= [10. 12.  0.] reward= -0.6400510571629023 done= False\n",
            "Step 8\n",
            "Action right\n",
            "obs= [10. 13.  0.] reward= -0.4050570233161105 done= False\n",
            "Step 9\n",
            "Action right\n",
            "obs= [10. 13.  0.] reward= -5.0 done= False\n",
            "Step 10\n",
            "Action right\n",
            "obs= [10. 13.  0.] reward= -5.0 done= False\n",
            "Step 11\n",
            "Action right\n",
            "obs= [ 9. 13.  0.] reward= -0.9542674354258024 done= False\n",
            "Step 12\n",
            "Action right\n",
            "obs= [ 9. 12.  0.] reward= -0.7408508419994274 done= False\n",
            "Step 13\n",
            "Action right\n",
            "obs= [ 9. 13.  0.] reward= -0.9945196382364473 done= False\n",
            "Step 14\n",
            "Action right\n",
            "obs= [ 9. 14.  0.] reward= -0.34690756064048767 done= False\n",
            "Step 15\n",
            "Action right\n",
            "obs= [ 9. 15.  0.] reward= -0.4341145990213551 done= False\n",
            "Step 16\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.8665308566726354 done= False\n",
            "Step 17\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.2166770480394723 done= False\n",
            "Step 18\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 19\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 20\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 21\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 22\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 23\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 24\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 25\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 26\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 27\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 28\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 29\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 30\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 31\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 32\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 33\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 34\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 35\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 36\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 37\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 38\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 39\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 40\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 41\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 42\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 43\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-108-0d4da59cd8ed>:166: UserWarning: \u001b[33mWARN: Render mode not implemented\u001b[0m\n",
            "  gym.logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 45\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.42740167502646487 done= False\n",
            "Step 46\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.9243753744576112 done= False\n",
            "Step 47\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 48\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 49\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 50\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 51\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 52\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 53\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 54\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 55\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 56\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 57\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.9429403730836579 done= False\n",
            "Step 58\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.1604541609933402 done= False\n",
            "Step 59\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 60\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 61\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 62\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 63\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 64\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 65\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 66\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 67\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 68\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 69\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 70\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 71\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 72\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 73\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 74\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 75\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 76\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 77\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 78\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 79\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 80\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 81\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 82\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 83\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 84\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 85\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 86\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 87\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 88\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 89\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 90\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 91\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 92\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 93\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 94\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 95\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 96\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 97\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -0.772356339814853 done= False\n",
            "Step 98\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 99\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 100\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 101\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 102\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 103\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 104\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 105\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 106\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 107\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 108\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 109\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 110\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 111\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.0747933586477767 done= False\n",
            "Step 112\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 113\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 114\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 115\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 116\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 117\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 118\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 119\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 120\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 121\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.6573861651535259 done= False\n",
            "Step 122\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.5008794668164892 done= False\n",
            "Step 123\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 124\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 125\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 126\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 127\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 128\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 129\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 130\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 131\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 132\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 133\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 134\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 135\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 136\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 137\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 138\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 139\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 140\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 141\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 142\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 143\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 144\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.6780552369238071 done= False\n",
            "Step 145\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.5647064149720085 done= False\n",
            "Step 146\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 147\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 148\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 149\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 150\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 151\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 152\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 153\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 154\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 155\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 156\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 157\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 158\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 159\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 160\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 161\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 162\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 163\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 164\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 165\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 166\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 167\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 168\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 169\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 170\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.07330679233313486 done= False\n",
            "Step 171\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.10264604870103744 done= False\n",
            "Step 172\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 173\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 174\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 175\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 176\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 177\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 178\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 179\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 180\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 181\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 182\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 183\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 184\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 185\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 186\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 187\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 188\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 189\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 190\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 191\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 192\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 193\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 194\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 195\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 196\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 197\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 198\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 199\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 200\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 201\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 202\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 203\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 204\n",
            "Action right\n",
            "obs= [ 9. 16.  0.] reward= -0.3088146953791836 done= False\n",
            "Step 205\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -0.8653522373374584 done= False\n",
            "Step 206\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 207\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 208\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 209\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 210\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 211\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 212\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 213\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 214\n",
            "Action right\n",
            "obs= [ 9. 17.  0.] reward= -5.0 done= False\n",
            "Step 215\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -0.6938022894433992 done= False\n",
            "Step 216\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 217\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 218\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 219\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 220\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 221\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 222\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 223\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 224\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 225\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 226\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 227\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 228\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 229\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 230\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 231\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 232\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 233\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 234\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 235\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 236\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 237\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 238\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 239\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 240\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 241\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 242\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 243\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 244\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 245\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 246\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 247\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 248\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 249\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 250\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 251\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 252\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 253\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 254\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 255\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 256\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 257\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 258\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 259\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 260\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 261\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 262\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 263\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 264\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 265\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 266\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 267\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 268\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 269\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 270\n",
            "Action right\n",
            "obs= [10. 17.  0.] reward= -5.0 done= False\n",
            "Step 271\n",
            "Action right\n",
            "obs= [11. 17.  0.] reward= -0.6837906500430957 done= False\n",
            "Step 272\n",
            "Action right\n",
            "obs= [11. 18.  0.] reward= -0.49301174442067064 done= False\n",
            "Step 273\n",
            "Action right\n",
            "obs= [11. 19.  0.] reward= -0.2656647982967342 done= False\n",
            "Step 274\n",
            "Action right\n",
            "obs= [11. 20.  0.] reward= -0.5860692262853295 done= False\n",
            "Step 275\n",
            "Action right\n",
            "obs= [11. 21.  0.] reward= -0.7387528289860552 done= False\n",
            "Step 276\n",
            "Action right\n",
            "obs= [11. 22.  0.] reward= -0.4763584667884383 done= False\n",
            "Step 277\n",
            "Action right\n",
            "obs= [11. 23.  0.] reward= -0.7409799509386078 done= False\n",
            "Step 278\n",
            "Action right\n",
            "obs= [11. 24.  0.] reward= -0.4407121241692137 done= False\n",
            "Step 279\n",
            "Action right\n",
            "obs= [11. 25.  0.] reward= -0.6093644735112969 done= False\n",
            "Step 280\n",
            "Action right\n",
            "obs= [11. 26.  0.] reward= -0.8990324337837837 done= False\n",
            "Step 281\n",
            "Action right\n",
            "obs= [11. 27.  0.] reward= -0.02780986836893473 done= False\n",
            "Step 282\n",
            "Action right\n",
            "obs= [11. 28.  0.] reward= -0.47629432377459313 done= False\n",
            "Step 283\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.214860253311565 done= False\n",
            "Step 284\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.4648960024921096 done= False\n",
            "Step 285\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.5759227957747483 done= False\n",
            "Step 286\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.5406276854338855 done= False\n",
            "Step 287\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.2865722840982574 done= False\n",
            "Step 288\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.9660247432702186 done= False\n",
            "Step 289\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.1618226365965899 done= False\n",
            "Step 290\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.6628791087828558 done= False\n",
            "Step 291\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.9733369149304191 done= False\n",
            "Step 292\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.7537516172247267 done= False\n",
            "Step 293\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.036907037980136836 done= False\n",
            "Step 294\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.4716526072551609 done= False\n",
            "Step 295\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.295517121992938 done= False\n",
            "Step 296\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.20883221420309195 done= False\n",
            "Step 297\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.04851736087911085 done= False\n",
            "Step 298\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.8258773610274478 done= False\n",
            "Step 299\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.4561612073516651 done= False\n",
            "Step 300\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.7861245529456312 done= False\n",
            "Step 301\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.021635498415528587 done= False\n",
            "Step 302\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.7471264283614769 done= False\n",
            "Step 303\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.8624194257059742 done= False\n",
            "Step 304\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.18735287774750098 done= False\n",
            "Step 305\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.6501003143868925 done= False\n",
            "Step 306\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.5206501821282096 done= False\n",
            "Step 307\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.312880223722139 done= False\n",
            "Step 308\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.05657503748167714 done= False\n",
            "Step 309\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.7857460108242283 done= False\n",
            "Step 310\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.9895480812511596 done= False\n",
            "Step 311\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.8402196317002761 done= False\n",
            "Step 312\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.03050232168366096 done= False\n",
            "Step 313\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.358857071676672 done= False\n",
            "Step 314\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.3919359915213474 done= False\n",
            "Step 315\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.7068175133200592 done= False\n",
            "Step 316\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.3131902604621225 done= False\n",
            "Step 317\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.2496045016709545 done= False\n",
            "Step 318\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.1771451152665109 done= False\n",
            "Step 319\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.8448700536460183 done= False\n",
            "Step 320\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.7473915920350919 done= False\n",
            "Step 321\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.540289769958313 done= False\n",
            "Step 322\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.8741044175444939 done= False\n",
            "Step 323\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.1997981288294931 done= False\n",
            "Step 324\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.6319567274581638 done= False\n",
            "Step 325\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.3567549538189455 done= False\n",
            "Step 326\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.5700897168844269 done= False\n",
            "Step 327\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.26582023584496317 done= False\n",
            "Step 328\n",
            "Action left\n",
            "obs= [10. 29.  0.] reward= -0.9168435403063735 done= False\n",
            "Step 329\n",
            "Action left\n",
            "obs= [10. 28.  0.] reward= -0.7946424115524641 done= False\n",
            "Step 330\n",
            "Action left\n",
            "obs= [10. 27.  0.] reward= -0.9583892359676672 done= False\n",
            "Step 331\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.40943578357000354 done= False\n",
            "Step 332\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.14163201220974198 done= False\n",
            "Step 333\n",
            "Action left\n",
            "obs= [11. 27.  0.] reward= -0.06321868021459265 done= False\n",
            "Step 334\n",
            "Action right\n",
            "obs= [11. 28.  0.] reward= -0.08143130638671159 done= False\n",
            "Step 335\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.6467179200300739 done= False\n",
            "Step 336\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.19658840663791055 done= False\n",
            "Step 337\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.34114447546850335 done= False\n",
            "Step 338\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.19608577975825592 done= False\n",
            "Step 339\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.48629638899929795 done= False\n",
            "Step 340\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.7819266891983194 done= False\n",
            "Step 341\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.7261781022928193 done= False\n",
            "Step 342\n",
            "Action left\n",
            "obs= [11. 28.  0.] reward= -0.9642792347669278 done= False\n",
            "Step 343\n",
            "Action right\n",
            "obs= [11. 29.  0.] reward= -0.6467715178259068 done= False\n",
            "Step 344\n",
            "Action left\n",
            "obs= [10. 29.  0.] reward= -0.03484901820833952 done= False\n",
            "Step 345\n",
            "Action left\n",
            "obs= [10. 28.  0.] reward= -0.1930726073937875 done= False\n",
            "Step 346\n",
            "Action left\n",
            "obs= [10. 27.  0.] reward= -0.25646375965755586 done= False\n",
            "Step 347\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.5820758664164747 done= False\n",
            "Step 348\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.47662483235414543 done= False\n",
            "Step 349\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.23183738440028423 done= False\n",
            "Step 350\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.46495000903651784 done= False\n",
            "Step 351\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.050009376836948616 done= False\n",
            "Step 352\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.03938310512080634 done= False\n",
            "Step 353\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.021316188634152522 done= False\n",
            "Step 354\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.5310131588644544 done= False\n",
            "Step 355\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.08832154710068141 done= False\n",
            "Step 356\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.04301474025954177 done= False\n",
            "Step 357\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.12116647343409426 done= False\n",
            "Step 358\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.9357607522444725 done= False\n",
            "Step 359\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.5376583227208969 done= False\n",
            "Step 360\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.9375598562872167 done= False\n",
            "Step 361\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.7632991160485295 done= False\n",
            "Step 362\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.11321768252442377 done= False\n",
            "Step 363\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.9710939685423202 done= False\n",
            "Step 364\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.8173287366149997 done= False\n",
            "Step 365\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.1384822020933747 done= False\n",
            "Step 366\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.4524230998414861 done= False\n",
            "Step 367\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.7305320677070977 done= False\n",
            "Step 368\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.565138644994487 done= False\n",
            "Step 369\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.4566050949887711 done= False\n",
            "Step 370\n",
            "Action right\n",
            "obs= [ 9. 26.  0.] reward= -0.3138407077812827 done= False\n",
            "Step 371\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.90232910224274 done= False\n",
            "Step 372\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 373\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 374\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 375\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 376\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 377\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 378\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 379\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 380\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 381\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.15817990464952636 done= False\n",
            "Step 382\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 383\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 384\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 385\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 386\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 387\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 388\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 389\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 390\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 391\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 392\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 393\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 394\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 395\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 396\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 397\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 398\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 399\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 400\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 401\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 402\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.2331732263212798 done= False\n",
            "Step 403\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 404\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 405\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 406\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 407\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 408\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 409\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 410\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 411\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 412\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 413\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 414\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 415\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 416\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 417\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 418\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 419\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 420\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 421\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 422\n",
            "Action left\n",
            "obs= [ 9. 26.  0.] reward= -0.49515459043488397 done= False\n",
            "Step 423\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.4494710345306169 done= False\n",
            "Step 424\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 425\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 426\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 427\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 428\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 429\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 430\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 431\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 432\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 433\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 434\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 435\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 436\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 437\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 438\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 439\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.3407353639582412 done= False\n",
            "Step 440\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 441\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 442\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 443\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 444\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 445\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 446\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 447\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 448\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 449\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 450\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 451\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 452\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 453\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.4767238485735332 done= False\n",
            "Step 454\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 455\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 456\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 457\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 458\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 459\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 460\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 461\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 462\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 463\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 464\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 465\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 466\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 467\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 468\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 469\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 470\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 471\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 472\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 473\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 474\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 475\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 476\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 477\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 478\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 479\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 480\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 481\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 482\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 483\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 484\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 485\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 486\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 487\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 488\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 489\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 490\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 491\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 492\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 493\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 494\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 495\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 496\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 497\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 498\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 499\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 500\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 501\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 502\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 503\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 504\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 505\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 506\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 507\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 508\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 509\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 510\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 511\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 512\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 513\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 514\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 515\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 516\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 517\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 518\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.19388929769974594 done= False\n",
            "Step 519\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.7457095918785909 done= False\n",
            "Step 520\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 521\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 522\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 523\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 524\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 525\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 526\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 527\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 528\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 529\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 530\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 531\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 532\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 533\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.5633212788699625 done= False\n",
            "Step 534\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 535\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 536\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 537\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.487495479540354 done= False\n",
            "Step 538\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 539\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 540\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 541\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 542\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 543\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.33055039110535456 done= False\n",
            "Step 544\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 545\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 546\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 547\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 548\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 549\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 550\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 551\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 552\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 553\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 554\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 555\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 556\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 557\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 558\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 559\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 560\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 561\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 562\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 563\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 564\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 565\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 566\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 567\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 568\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 569\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 570\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 571\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 572\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 573\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 574\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 575\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 576\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 577\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 578\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 579\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 580\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 581\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 582\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 583\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 584\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 585\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 586\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 587\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 588\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 589\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 590\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 591\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 592\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 593\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 594\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 595\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 596\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 597\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 598\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 599\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 600\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.7088154410066602 done= False\n",
            "Step 601\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 602\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 603\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.5350475980617188 done= False\n",
            "Step 604\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 605\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 606\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 607\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 608\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 609\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 610\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 611\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 612\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.9878266693017655 done= False\n",
            "Step 613\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 614\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 615\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 616\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.047875091751257814 done= False\n",
            "Step 617\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 618\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 619\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 620\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 621\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 622\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 623\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 624\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 625\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 626\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 627\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 628\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 629\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 630\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 631\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 632\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 633\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 634\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 635\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 636\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 637\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 638\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 639\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 640\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 641\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 642\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 643\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 644\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 645\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 646\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 647\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 648\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 649\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 650\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 651\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 652\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 653\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 654\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 655\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 656\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 657\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 658\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 659\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 660\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 661\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 662\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 663\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 664\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 665\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 666\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 667\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 668\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 669\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 670\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 671\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 672\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 673\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 674\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 675\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 676\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 677\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 678\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 679\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 680\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 681\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 682\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.6132074240613314 done= False\n",
            "Step 683\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 684\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 685\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 686\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 687\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 688\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 689\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 690\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 691\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 692\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 693\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 694\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 695\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 696\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 697\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 698\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 699\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 700\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 701\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 702\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 703\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 704\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 705\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 706\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 707\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 708\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 709\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 710\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.3178768980488744 done= False\n",
            "Step 711\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 712\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 713\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 714\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 715\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 716\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 717\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 718\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 719\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 720\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 721\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 722\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 723\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 724\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 725\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 726\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 727\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 728\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 729\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 730\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 731\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 732\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 733\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 734\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 735\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 736\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 737\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 738\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 739\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 740\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 741\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 742\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 743\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 744\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 745\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 746\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 747\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 748\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 749\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 750\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 751\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 752\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.10113063246578424 done= False\n",
            "Step 753\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 754\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 755\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 756\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 757\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 758\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 759\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 760\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 761\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 762\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 763\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 764\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 765\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 766\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 767\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 768\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 769\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 770\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 771\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 772\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 773\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 774\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 775\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.4121859319858463 done= False\n",
            "Step 776\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 777\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 778\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 779\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 780\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 781\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 782\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 783\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 784\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 785\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 786\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 787\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 788\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 789\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 790\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 791\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 792\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 793\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 794\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 795\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 796\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.9054592937866154 done= False\n",
            "Step 797\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 798\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 799\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 800\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 801\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 802\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 803\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 804\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 805\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 806\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 807\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 808\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.06104255924822943 done= False\n",
            "Step 809\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 810\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 811\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 812\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 813\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 814\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 815\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 816\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 817\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 818\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 819\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 820\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -5.0 done= False\n",
            "Step 821\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -0.2517739437288483 done= False\n",
            "Step 822\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 823\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 824\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= 1000.0 done= False\n",
            "Step 825\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 826\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 827\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 828\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 829\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 830\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 831\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 832\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 833\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 834\n",
            "Action left\n",
            "obs= [ 7. 25.  0.] reward= -5.0 done= False\n",
            "Step 835\n",
            "Action left\n",
            "obs= [ 8. 25.  0.] reward= -0.47398914910559065 done= False\n",
            "Step 836\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.15933463465269315 done= False\n",
            "Step 837\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 838\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 839\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 840\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 841\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 842\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 843\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 844\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 845\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 846\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 847\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 848\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 849\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 850\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 851\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 852\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 853\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 854\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 855\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 856\n",
            "Action left\n",
            "obs= [10. 25.  0.] reward= -0.954042732546636 done= False\n",
            "Step 857\n",
            "Action right\n",
            "obs= [10. 26.  0.] reward= -0.09800028998379418 done= False\n",
            "Step 858\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.838490257838868 done= False\n",
            "Step 859\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.7568477179599493 done= False\n",
            "Step 860\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.10125935512128081 done= False\n",
            "Step 861\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.22525691474374576 done= False\n",
            "Step 862\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.49926499372692157 done= False\n",
            "Step 863\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.484188100554351 done= False\n",
            "Step 864\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.6943176293370882 done= False\n",
            "Step 865\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.8510618699444945 done= False\n",
            "Step 866\n",
            "Action right\n",
            "obs= [10. 27.  0.] reward= -0.021231206826598847 done= False\n",
            "Step 867\n",
            "Action left\n",
            "obs= [10. 26.  0.] reward= -0.0563681125768124 done= False\n",
            "Step 868\n",
            "Action right\n",
            "obs= [ 9. 26.  0.] reward= -0.8293514427451615 done= False\n",
            "Step 869\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.08141189623656764 done= False\n",
            "Step 870\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 871\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 872\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 873\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 874\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 875\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 876\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 877\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 878\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 879\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 880\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 881\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 882\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 883\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 884\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 885\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 886\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 887\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 888\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 889\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 890\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 891\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 892\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 893\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 894\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 895\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 896\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 897\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 898\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 899\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 900\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 901\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 902\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 903\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 904\n",
            "Action left\n",
            "obs= [ 9. 26.  0.] reward= -0.0668029870028729 done= False\n",
            "Step 905\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -0.6016909082856609 done= False\n",
            "Step 906\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 907\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 908\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 909\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= False\n",
            "Step 910\n",
            "Action left\n",
            "obs= [ 9. 25.  0.] reward= -5.0 done= True\n",
            "Goal reached! reward= -5.0\n",
            "[array([10.,  7.,  0.], dtype=float32), array([10.,  8.,  0.], dtype=float32), array([10.,  9.,  0.], dtype=float32), array([10., 10.,  0.], dtype=float32), array([10.,  9.,  0.], dtype=float32), array([10., 10.,  0.], dtype=float32), array([10., 11.,  0.], dtype=float32), array([10., 12.,  0.], dtype=float32), array([10., 13.,  0.], dtype=float32), array([10., 13.,  0.], dtype=float32), array([10., 13.,  0.], dtype=float32), array([ 9., 13.,  0.], dtype=float32), array([ 9., 12.,  0.], dtype=float32), array([ 9., 13.,  0.], dtype=float32), array([ 9., 14.,  0.], dtype=float32), array([ 9., 15.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 16.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([ 9., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([10., 17.,  0.], dtype=float32), array([11., 17.,  0.], dtype=float32), array([11., 18.,  0.], dtype=float32), array([11., 19.,  0.], dtype=float32), array([11., 20.,  0.], dtype=float32), array([11., 21.,  0.], dtype=float32), array([11., 22.,  0.], dtype=float32), array([11., 23.,  0.], dtype=float32), array([11., 24.,  0.], dtype=float32), array([11., 25.,  0.], dtype=float32), array([11., 26.,  0.], dtype=float32), array([11., 27.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([10., 29.,  0.], dtype=float32), array([10., 28.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([11., 27.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([11., 28.,  0.], dtype=float32), array([11., 29.,  0.], dtype=float32), array([10., 29.,  0.], dtype=float32), array([10., 28.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([ 9., 26.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 26.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 7., 25.,  0.], dtype=float32), array([ 8., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([10., 25.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([10., 27.,  0.], dtype=float32), array([10., 26.,  0.], dtype=float32), array([ 9., 26.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 26.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32), array([ 9., 25.,  0.], dtype=float32)]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Test the trained agent\n",
        "# using the vecenv\n",
        "# obs = vec_env.reset()\n",
        "# n_steps = 2000\n",
        "# for step in range(n_steps):\n",
        "#     action, _ = model.predict(obs, deterministic=True)\n",
        "#     print(f\"Step {step + 1}\")\n",
        "#     print(\"Action: \", action)\n",
        "#     obs, reward, done, info = vec_env.step(action)\n",
        "#     print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "#     vec_env.render()\n",
        "#     if done:\n",
        "#         # Note that the VecEnv resets automatically\n",
        "#         # when a done signal is encountered\n",
        "#         print(\"Goal reached!\", \"reward=\", reward)\n",
        "#         break\n",
        "\n",
        "\n",
        "obs = vec_env.reset()[0]\n",
        "actions = ['up','down','left','right']\n",
        "# Hardcoded best agent: always go left!\n",
        "n_steps = 5000\n",
        "state_history = []\n",
        "state_history.append(obs)\n",
        "for step in range(n_steps):\n",
        "    print(f\"Step {step + 1}\")\n",
        "    action = model.predict(obs, deterministic=True)\n",
        "    print(f\"Action {actions[action[0]]}\")\n",
        "    obs, reward,terminated, truncated, info = vec_env.envs[0].step(action[0])\n",
        "    state_history.append(obs)\n",
        "    done = terminated or truncated\n",
        "    print(\"obs=\", obs, \"reward=\", reward, \"done=\", done)\n",
        "    vec_env.envs[0].render()\n",
        "    if done:\n",
        "        print(\"Goal reached!\", \"reward=\", reward)\n",
        "        break\n",
        "\n",
        "print(state_history)\n",
        "vec_env.envs[0].show_path(np.array(state_history))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "RuJDXFmkB-rM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}